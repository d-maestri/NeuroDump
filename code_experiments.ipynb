{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1cbf21",
   "metadata": {},
   "source": [
    "# clean the Chroma DB:\n",
    "-  on terminal:\n",
    "rm -rf ./data/chroma_db\n",
    "- delete list from \"processed_files.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d34171",
   "metadata": {},
   "source": [
    "# First experiment: Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(model: str, prompt: str) -> str:\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt.encode('utf-8'),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "    return result.stdout.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e2131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here's the organized and rewritten version of your text according to your instructions:\n",
      "\n",
      "[Idea]\n",
      "- I had an idea for a new project.\n",
      "\n",
      "[Task]\n",
      "- Later I thought maybe I should buy cat food.\n",
      "\n",
      "[Message]\n",
      "- [Idea: Sharing my idea about a new project]\n",
      "- [Task: Suggesting that Alex might help with buying cat food]\n",
      "\n",
      "Rewritten and Reordered version:\n",
      "\n",
      "1. First, I'd like to share an idea I had for a new project - do you think you could help discuss it?\n",
      "2. Later, I thought it would be good if we could buy some cat food together as I need to take care of my pet. Let me know what you think.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I had an idea for a new project. Also I need to buy cat food. \n",
    "Later I thought maybe I should send that message to Alex.\n",
    "\n",
    "1. Group related sentences.\n",
    "2. Label each group as: [Idea, Task, Message]\n",
    "3. Reorder and rewrite each clearly.\n",
    "\"\"\"\n",
    "\n",
    "response = query_ollama(\"mistral\", prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c360c",
   "metadata": {},
   "source": [
    "It didn't work exactly how I expected and it took too long to give me an answer. Prompt engineering refinement, but still, hard to test. Changing approach to classical NLP and using generative AI models only for rewriting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08bdc6",
   "metadata": {},
   "source": [
    "# word tokenize + get chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe496622",
   "metadata": {},
   "source": [
    "- Analysis: The [word_toneknize] splits the punctiation into separate tokens and creates this view that makes harder to compreehend the context. I researched other tokenizer and found a \"de-tokenizer\" [tree bank word] to join the tokens in a more natural way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to split documents with Word+Tokenize ###\n",
    "\n",
    "def split_by_words(text, chunk_size=80, overlap=20):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "# 2. Split only new files into chunks of words\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "else:\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    for fname in new_files:\n",
    "        with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            display(f\"Text file: {fname}\", Markdown(f\"```\\n{text}\\n```\"))\n",
    "        chunks = split_by_words(text)\n",
    "        all_chunks.extend(chunks)\n",
    "        all_metadatas.extend([{\"source\": fname}] * len(chunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33295bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 chunks from note-1-dt-r.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 ( p ) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster ? less bias ? not 100 % sure . trees go deep and then prone back ? no , prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right ? sklearn.tree.DecisionTreeClassifier ( max_depth=3 ) — yeah that ’ s what prof used . note : good to visualize trees but not for high dim data . lots of axis-aligned splits , hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum ( w² ) , shrinks weights , but all stay ≠ 0 . Lasso adds λ * sum ( |w| ) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board : from sklearn.linear_model import Lasso model = Lasso ( alpha=0.1 ) model.fit ( X_train , y_train ) ElasticNet = mix of both ? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting .\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible model class I think ?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 ( p ) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster ? less bias ? not 100 % sure . trees go deep and then prone back ? no , prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right ? sklearn.tree.DecisionTreeClassifier ( max_depth=3 ) — yeah that ’ s what prof used . note : good to visualize trees but not for high dim data . lots of axis-aligned splits , hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum ( w² ) , shrinks weights , but all stay ≠ 0 . Lasso adds λ * sum ( |w| ) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board : from sklearn.linear_model import Lasso model = Lasso ( alpha=0.1 ) model.fit ( X_train , y_train ) ElasticNet = mix of both ? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting .\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible model class I think ?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### get the chunks from a specific file\n",
    "\n",
    "def get_file_chunks(fname):\n",
    "    results = vectorstore.get(where={\"source\": fname})\n",
    "    file_chunks = results['documents']\n",
    "    print(f\"Found {len(file_chunks)} chunks from {fname}\")\n",
    "    for chunk in file_chunks:\n",
    "        display(Markdown(f\"```\\n{chunk}\\n```\"))\n",
    "\n",
    "file1_chunks = get_file_chunks(\"note-1-dt-r.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0aca9",
   "metadata": {},
   "source": [
    "### 2. Document segmentation (text split)\n",
    "Split the document into chunks of 80 words, which is enough to capture the semantic context. Sentences and words would miss the context. Paragraphs are not consistent with unstructured documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to split document with TreebankWordDetokenizer ###\n",
    "\n",
    "def split_by_words_de(text, chunk_size=80, overlap=20):\n",
    "    words = word_tokenize(text)\n",
    "    detok = TreebankWordDetokenizer()                       # detokenizer to join the tokens in a more natural way\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(detok.detokenize(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a554f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original file: note-3-dt-r-ul.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "lecture today was fast af. started with trees. entropy vs gini impurity — diff metrics to decide best split. both OK. CART = binary tree = each node has 2 splits. sklearn uses this. code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
       "trees prone to overfit — esp if depth unbounded. pruning = way to fix. early stopping or post-prune. bagging helps = ensemble. RandomForest = multiple trees on bootstrapped samples + rand subset of features per split. reduces variance.\n",
       "trees = interpretability good, but unstable to data changes.\n",
       "then prof jumped into regression. regularization = shrink model capacity. Ridge = L2 norm = λ * Σ(w²). Lasso = L1 norm = λ * Σ|w|. Ridge keeps all weights ≠ 0, Lasso can zero out → sparse. Lasso good for feature selection. ElasticNet = mix of both — good if features correlated\n",
       "code ex:\n",
       "from sklearn.linear_model import ElasticNet model = ElasticNet(alpha=0.1, l1_ratio=0.5) model.fit(X, y)\n",
       "important: scale features before fitting regularized models — otherwise magnitudes skew the penalty. StandardScaler or RobustScaler if outliers.\n",
       "tune α via cross-val — use GridSearchCV or RandomizedSearchCV.\n",
       "metrics: RMSE, R². underfit vs overfit — regularization helps balance bias/var tradeoff.\n",
       "last part was unsupervised learning. clustering w/o labels. k-means = most used. init centers, assign pts, recalc, repeat. problem: sensitive to init. use k-means++. elbow method not always clear. silhouette score better maybe.\n",
       "PCA + k-means often combined for vis. t-SNE only for viz — not for modeling. clusters in t-SNE are sometimes fake.\n",
       "DBSCAN = cluster via density. can detect noise. great for shape-agnostic clusters. params hard to tune tho. hierarchical = dendrograms. use ‘ward’ linkage. but slow w/ big data.\n",
       "general note: sklearn models consistent API — fit / predict / score.\n",
       "also: why trees don’t need scaling? bcz splits based on order not value. contrast w/ reg models.\n",
       "each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note-3-dt-r-ul.txt = 7 chunks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "lecture today was fast af . started with trees . entropy vs gini impurity — diff metrics to decide best split . both OK. CART = binary tree = each node has 2 splits . sklearn uses this . code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier (criterion='entropy', max_depth=4) trees prone to overfit — esp if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       "if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble . RandomForest = multiple trees on bootstrapped samples + rand subset of features per split . reduces variance . trees = interpretability good, but unstable to data changes . then prof jumped into regression . regularization = shrink model capacity . Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ *)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       ". Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ * Σ|w| . Ridge keeps all weights ≠ 0, Lasso can zero out → sparse . Lasso good for feature selection . ElasticNet = mix of both — good if features correlated code ex: from sklearn.linear_model import ElasticNet model = ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:**\n",
       "= ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized models — otherwise magnitudes skew the penalty . StandardScaler or RobustScaler if outliers . tune α via cross-val — use GridSearchCV or RandomizedSearchCV . metrics: RMSE, R² . underfit vs overfit — regularization helps balance bias/var tradeoff . last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:**\n",
       "last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts, recalc, repeat . problem: sensitive to init . use k-means++ . elbow method not always clear . silhouette score better maybe . PCA + k-means often combined for vis . t-SNE only for viz — not for modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:**\n",
       "modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great for shape-agnostic clusters . params hard to tune tho . hierarchical = dendrograms . use ‘ ward ’ linkage . but slow w/ big data . general note: sklearn models consistent API — fit / predict / score . also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 7:**\n",
       "also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/ reg models . each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Original file: note-1-dt-r.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2(p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node. Gini impurity also similar but faster? less bias? not 100% sure. trees go deep and then prone back? no, prune. to prevent overfitting. training error low but generalization bad. CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier(max_depth=3) — yeah that’s what prof used.\n",
       "note: good to visualize trees but not for high dim data. lots of axis-aligned splits, hard to interpret when too many features. oh and trees are unstable — small data change = big model change. they said bagging can help that — RandomForest.\n",
       "wait then they jumped to regularization — lasso vs ridge. ridge adds λ * sum(w²), shrinks weights, but all stay ≠ 0. Lasso adds λ * sum(|w|) — forces some to zero. ohhh good for feature selection. balance bias-variance tradeoff. λ too big = underfit. low λ = overfit. prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso(alpha=0.1) model.fit(X_train, y_train)\n",
       "ElasticNet = mix of both? ratio param controls mix. good when multicollinearity or many small coeffs. visualize loss function — lasso diamond corners cause zeros. interesting.\n",
       "btw they said don’t scale trees but do scale for lasso etc. bcz regularization depends on magnitude. std scaling or minmax okay.\n",
       "might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV\n",
       "decision boundary of tree is step-like, not smooth like linear models.\n",
       "ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note-1-dt-r.txt = 6 chunks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:**\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:**\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:**\n",
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Original file: note-2-ul.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "unsup = no labels. kmeans = simplest one but still used a lot. init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat. converge when no pt changes. but result depends on init + scale. scale important! feature w bigger range dominates dist calc — always standardize first.\n",
       "elbow method = plot inertia vs k — look for bend, but not always obvious. inertia = sum of dist² to centroid. alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered.\n",
       "clustering ≠ classification. labels are not known. use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples. tricky to tune tho. forms clusters based on density, noisy pts marked as outliers (label -1). sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN(eps=0.5, min_samples=5) model.fit(X)\n",
       "hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w all pts as indiv cluster, merge closest pairs step by step. dendrogram = tree of merges. can \"cut\" tree at diff levels = diff num clusters. linkage: single, complete, avg. sklearn has AgglomerativeClustering.\n",
       "before clustering, can reduce dim (PCA) for speed + viz. tSNE/UMAP for 2D plot = better for human eye but not for modeling. PCA = linear, tSNE = non-linear. tSNE distorts structure globally. good for pattern discovery.\n",
       "spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace. nice when structure is graphy, not spherical.\n",
       "most clustering algos rely on distance metric — Euclidean default. alt: cosine sim (for text), manhattan, etc.\n",
       "eval: hard bcz no true label. silhouette best for most. DB index too. compare within/between cluster distance. can also visualize clusters to judge quality.\n",
       "problem: k-means assumes spherical clusters, equal size. not true for real-world. if data has diff density or shapes → fails.\n",
       "open Q: how to know if clusters mean anything in real world?\n",
       "scaling is essential — StandardScaler or MinMaxScaler from sklearn.\n",
       "pipeline ex:\n",
       "from sklearn.pipeline import make_pipeline pipe = make_pipeline(StandardScaler(), KMeans(n_clusters=3)) pipe.fit(X)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note-2-ul.txt = 8 chunks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "unsup = no labels . kmeans = simplest one but still used a lot . init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat . converge when no pt changes . but result depends on init + scale . scale important! feature w bigger range dominates dist calc — always standardize first . elbow method = plot inertia vs k — look for bend, but not always obvious . inertia =)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       ". elbow method = plot inertia vs k — look for bend, but not always obvious . inertia = sum of dist² to centroid . alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered . clustering ≠ classification . labels are not known . use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps +)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       ", anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples . tricky to tune tho . forms clusters based on density, noisy pts marked as outliers (label -1). sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN (eps=0.5, min_samples=5) model.fit (X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:**\n",
       "(X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w all pts as indiv cluster, merge closest pairs step by step . dendrogram = tree of merges . can \"cut\" tree at diff levels = diff num clusters . linkage: single, complete, avg . sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:**\n",
       "sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for 2D plot = better for human eye but not for modeling . PCA = linear, tSNE = non-linear . tSNE distorts structure globally . good for pattern discovery . spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:**\n",
       "→ k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on distance metric — Euclidean default . alt: cosine sim (for text), manhattan, etc . eval: hard bcz no true label . silhouette best for most . DB index too . compare within/between cluster distance . can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 7:**\n",
       ". can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not true for real-world . if data has diff density or shapes → fails . open Q: how to know if clusters mean anything in real world? scaling is essential — StandardScaler or MinMaxScaler from sklearn . pipeline ex: from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit ()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 8:**\n",
       ": from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit (X))\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Split document into chunks of words\n",
    "\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "else:\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    for fname in new_files:\n",
    "        with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    \n",
    "        chunks = split_by_words_de(text)                                    #split in chunks\n",
    "        display(f\"Original file: {fname}\", Markdown(f\"```\\n{text}\\n```\"))   #display original file(s)\n",
    "        print(f\"{fname} = {len(chunks)} chunks\")                            #display number of chunks per file\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            display(Markdown(f\"**Chunk {i}:**\\n{chunk})\\n\"))\n",
    "        all_chunks.extend(chunks)\n",
    "        all_metadatas.extend([{\"source\": fname}] * len(chunks))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f4920",
   "metadata": {},
   "source": [
    "### 3. Embedding new chunks from new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/9xl167095gn7qzk16djnlv300000gp/T/ipykernel_34806/884519275.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Chroma DB in: ./data/chroma_db\n",
      "Processed and added 3 new files to Chroma DB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/9xl167095gn7qzk16djnlv300000gp/T/ipykernel_34806/884519275.py:23: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# 3.1. Embed new chunks\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# 3.2. Add embeddings to Chroma DB\n",
    "persist_dir = \"./data/chroma_db\"\n",
    "\n",
    "if os.path.exists(persist_dir):\n",
    "    print(f\"Loading existing Chroma DB from: {persist_dir}\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    vectorstore.add_texts(texts=all_chunks, metadatas=all_metadatas)\n",
    "else:\n",
    "    print(f\"Creating new Chroma DB in: {persist_dir}\")\n",
    "    vectorstore = Chroma.from_texts(\n",
    "        texts=all_chunks, \n",
    "        metadatas=all_metadatas, \n",
    "        embedding=embeddings, \n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "vectorstore.persist()\n",
    "\n",
    "\n",
    "# 3.3. Update processed files log\n",
    "with open(log_path, \"a\") as f:\n",
    "    for fname in new_files:\n",
    "        f.write(fname + \"\\n\")\n",
    "print(f\"Processed and added {len(new_files)} new files to Chroma DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4fa6a",
   "metadata": {},
   "source": [
    "### 4. Clusterize by semantic similarity\n",
    "1 exprimente: HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications with Noise -> not good. moved to other file\n",
    "2 experiment: kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to retrieve the chunks from a specific file\n",
    "\n",
    "def get_file_chunks(fname):\n",
    "    results = vectorstore.get(where={\"source\": fname})\n",
    "    file_chunks = results['documents']\n",
    "    print(f\"Found {len(file_chunks)} chunks from {fname}\")\n",
    "    for chunk in file_chunks:\n",
    "        display(Markdown(f\"```\\n{chunk}\\n```\"))             # display the contents of each chunk for conference\n",
    "    return file_chunks                                      # return the list for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 chunks from note-1-dt-r.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.1. Get embeddings chunks from File 1\n",
    "\n",
    "file1_chunks = get_file_chunks(\"note-1-dt-r.txt\")\n",
    "file1_embeddings = embeddings.embed_documents(file1_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans cluster labels: [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 2  # or 1, 2, 3 depending on your expectation\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(file1_embeddings)\n",
    "print(\"KMeans cluster labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b7bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cluster 0 ---\n",
      "Chunk 1:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 2:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 3:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big ="
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Cluster 1 ---\n",
      "Chunk 4:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 5:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 6:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Function to display chunks in clusters  ###\n",
    "\n",
    "for cluster_id in set(labels):\n",
    "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == cluster_id:\n",
    "            print(f\"Chunk {i+1}:\")\n",
    "            #print(file1_chunks[i]) \n",
    "            display(Markdown(file1_chunks[i]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7802adc",
   "metadata": {},
   "source": [
    "chat gpt splitting: \n",
    "1. entropy = -p * log2(p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node. Gini impurity also similar but faster? less bias? not 100% sure. trees go deep and then prone back? no, prune. to prevent overfitting. training error low but generalization bad. CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier(max_depth=3) — yeah that’s what prof used.\n",
    "note: good to visualize trees but not for high dim data. lots of axis-aligned splits, hard to interpret when too many features. oh and trees are unstable — small data change = big model change. they said bagging can help that — RandomForest.\n",
    "decision boundary of tree is step-like, not smooth like linear models.\n",
    "ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
    "btw they said don’t scale trees\n",
    "\n",
    "2. wait then they jumped to regularization — lasso vs ridge. ridge adds λ * sum(w²), shrinks weights, but all stay ≠ 0. Lasso adds λ * sum(|w|) — forces some to zero. ohhh good for feature selection. balance bias-variance tradeoff. λ too big = underfit. low λ = overfit. prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso(alpha=0.1) model.fit(X_train, y_train)\n",
    "ElasticNet = mix of both? ratio param controls mix. good when multicollinearity or many small coeffs. visualize loss function — lasso diamond corners cause zeros. interesting.\n",
    "btw they said […] do scale for lasso etc. bcz regularization depends on magnitude. std scaling or minmax okay.\n",
    "might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.54851236 0.37322338 0.11788947 0.25886513 0.30239811]\n",
      " [0.54851236 1.         0.44631097 0.33833166 0.44378826 0.47022212]\n",
      " [0.37322338 0.44631097 1.         0.49479337 0.6547976  0.52447321]\n",
      " [0.11788947 0.33833166 0.49479337 1.         0.59648798 0.45439273]\n",
      " [0.25886513 0.44378826 0.6547976  0.59648798 1.         0.62814475]\n",
      " [0.30239811 0.47022212 0.52447321 0.45439273 0.62814475 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(file1_embeddings)  # embeddings: list of vectors\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Interpretation:\n",
    "# 1.0: Vectors are identical in direction (maximum similarity).\n",
    "# 0.0: Vectors are orthogonal (no similarity).\n",
    "# -1.0: Vectors are opposite in direction (rare in NLP embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9609ecd",
   "metadata": {},
   "source": [
    "# experiment HDBSCAN:\n",
    "the results were -1, which means that everything was considered noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to retrieve the chunks from a specific file\n",
    "\n",
    "def get_file_chunks(fname):\n",
    "    results = vectorstore.get(where={\"source\": fname})\n",
    "    file_chunks = results['documents']\n",
    "    print(f\"Found {len(file_chunks)} chunks from {fname}\")\n",
    "    for chunk in file_chunks:\n",
    "        display(Markdown(f\"```\\n{chunk}\\n```\"))             # <-- display the contents of each chunk for conference\n",
    "    return file_chunks                                      # <-- return the list for further use\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e4549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 chunks from note-1-dt-r.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for File 1: [-1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# 4.1. Get embeddings chunks from File 1\n",
    "\n",
    "file1_chunks = get_file_chunks(\"note-1-dt-r.txt\")\n",
    "file1_embeddings = embeddings.embed_documents(file1_chunks)\n",
    "\n",
    "\n",
    "# 4.2. Initialize and fit HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2)\n",
    "labels = cluster.fit_predict(file1_embeddings)\n",
    "\n",
    "print(\"Cluster labels for File 1:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 chunks from note-2-ul.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "unsup = no labels . kmeans = simplest one but still used a lot . init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat . converge when no pt changes . but result depends on init + scale . scale important! feature w bigger range dominates dist calc — always standardize first . elbow method = plot inertia vs k — look for bend, but not always obvious . inertia =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". elbow method = plot inertia vs k — look for bend, but not always obvious . inertia = sum of dist² to centroid . alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered . clustering ≠ classification . labels are not known . use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps +\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ", anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples . tricky to tune tho . forms clusters based on density, noisy pts marked as outliers (label -1). sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN (eps=0.5, min_samples=5) model.fit (X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "(X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w all pts as indiv cluster, merge closest pairs step by step . dendrogram = tree of merges . can \"cut\" tree at diff levels = diff num clusters . linkage: single, complete, avg . sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for 2D plot = better for human eye but not for modeling . PCA = linear, tSNE = non-linear . tSNE distorts structure globally . good for pattern discovery . spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "→ k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on distance metric — Euclidean default . alt: cosine sim (for text), manhattan, etc . eval: hard bcz no true label . silhouette best for most . DB index too . compare within/between cluster distance . can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not true for real-world . if data has diff density or shapes → fails . open Q: how to know if clusters mean anything in real world? scaling is essential — StandardScaler or MinMaxScaler from sklearn . pipeline ex: from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit (\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ": from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit (X)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for File 2: [-1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "file2_chunks = get_file_chunks(\"note-2-ul.txt\")\n",
    "file2_embeddings = embeddings.embed_documents(file1_chunks)\n",
    "\n",
    "\n",
    "# 4.2. Initialize and fit HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2)\n",
    "labels = cluster.fit_predict(file2_embeddings)\n",
    "\n",
    "print(\"Cluster labels for File 2:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 chunks from note-3-dt-r-ul.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "lecture today was fast af . started with trees . entropy vs gini impurity — diff metrics to decide best split . both OK. CART = binary tree = each node has 2 splits . sklearn uses this . code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier (criterion='entropy', max_depth=4) trees prone to overfit — esp if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble . RandomForest = multiple trees on bootstrapped samples + rand subset of features per split . reduces variance . trees = interpretability good, but unstable to data changes . then prof jumped into regression . regularization = shrink model capacity . Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ *\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ * Σ|w| . Ridge keeps all weights ≠ 0, Lasso can zero out → sparse . Lasso good for feature selection . ElasticNet = mix of both — good if features correlated code ex: from sklearn.linear_model import ElasticNet model = ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "= ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized models — otherwise magnitudes skew the penalty . StandardScaler or RobustScaler if outliers . tune α via cross-val — use GridSearchCV or RandomizedSearchCV . metrics: RMSE, R² . underfit vs overfit — regularization helps balance bias/var tradeoff . last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts, recalc, repeat . problem: sensitive to init . use k-means++ . elbow method not always clear . silhouette score better maybe . PCA + k-means often combined for vis . t-SNE only for viz — not for modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great for shape-agnostic clusters . params hard to tune tho . hierarchical = dendrograms . use ‘ ward ’ linkage . but slow w/ big data . general note: sklearn models consistent API — fit / predict / score . also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/ reg models . each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for File 1: [-1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "file3_chunks = get_file_chunks(\"note-3-dt-r-ul.txt\")\n",
    "file3_embeddings = embeddings.embed_documents(file3_chunks)\n",
    "\n",
    "\n",
    "# 4.2. Initialize and fit HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2)\n",
    "labels = cluster.fit_predict(file3_embeddings)\n",
    "\n",
    "print(\"Cluster labels for File 1:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed0279",
   "metadata": {},
   "source": [
    "### old class clusterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00110134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkClusterizer:\n",
    "    def __init__(self, persist_dir, embeddings):\n",
    "        self.persist_dir = persist_dir\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = []\n",
    "        self.labels = []\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=self.persist_dir,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "    def get_file_chunks(self, fname):\n",
    "        results = self.vectorstore.get(where={\"source\": fname})\n",
    "        self.chunks = results['documents']\n",
    "        print(f\"Found {len(self.chunks)} chunks in {fname}\")\n",
    "        for chunk in self.chunks:\n",
    "            display(Markdown(f\"```\\n{chunk}\\n```\"))\n",
    "        return self.chunks\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=2):\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        X = self.embeddings.embed_documents(self.chunks)\n",
    "        clusterizer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        self.labels = clusterizer.fit_predict(X)\n",
    "\n",
    "        for cluster_id in sorted(set(self.labels)):\n",
    "            print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "            for i, label in enumerate(self.labels):\n",
    "                if label == cluster_id:\n",
    "                    print(f\"Chunk {i+1}:\\n{self.chunks[i][:200]}{'...' if len(self.chunks[i]) > 200 else ''}\\n\")\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99faec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings):\n",
    "        self.chunks = chunks\n",
    "        self.embeddings = np.array(chunk_embeddings) if chunk_embeddings else None\n",
    "        self.labels = []\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        \n",
    "        if not self.chunks:\n",
    "            print(\"No chunks to cluster.\")\n",
    "            return []\n",
    "            \n",
    "        if len(self.chunks) < min_cluster_size:\n",
    "            print(f\"Not enough chunks to cluster. Need at least {min_cluster_size}, but have {len(self.chunks)}.\")\n",
    "            self.labels = [-1] * len(self.chunks)\n",
    "            return self.labels\n",
    "\n",
    "        # Fix: Check numpy array properly\n",
    "        if self.embeddings is None or self.embeddings.size == 0:\n",
    "            print(\"No embeddings available for clustering.\")\n",
    "            self.labels = [-1] * len(self.chunks)\n",
    "            return self.labels\n",
    "\n",
    "        try:\n",
    "            # Ensure embeddings is a 2D array\n",
    "            if self.embeddings.ndim == 1:\n",
    "                self.embeddings = self.embeddings.reshape(1, -1)\n",
    "            \n",
    "            print(f\"Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "            clusterizer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size, \n",
    "                min_samples=min_samples,\n",
    "                metric='euclidean'\n",
    "            )\n",
    "            self.labels = clusterizer.fit_predict(self.embeddings)\n",
    "\n",
    "            num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n",
    "            print(f\"\\nFound {num_clusters} clusters in the document.\")\n",
    "            print(f\"Cluster distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "            \n",
    "            for cluster_id in sorted(set(self.labels)):\n",
    "                cluster_name = \"Noise (ungrouped chunks)\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "                chunk_count = sum(1 for label in self.labels if label == cluster_id)\n",
    "                print(f\"\\n--- {cluster_name} ({chunk_count} chunks) ---\")\n",
    "                \n",
    "                for i, label in enumerate(self.labels):\n",
    "                    if label == cluster_id:\n",
    "                        preview = self.chunks[i][:200] + ('...' if len(self.chunks[i]) > 200 else '')\n",
    "                        print(f\"Chunk {i+1}: {preview}\\n\")\n",
    "            \n",
    "            return self.labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during clustering: {e}\")\n",
    "            self.labels = [-1] * len(self.chunks)\n",
    "            return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09a9da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b288475b",
   "metadata": {},
   "source": [
    "# code backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6059a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ecfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ecc42",
   "metadata": {},
   "source": [
    "### 1. Input unstructured document\n",
    "Incremental version. It will process only new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0593269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config paths\n",
    "\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          #files\n",
    "LOG_PATH = \"./data/processed_files.txt\"     #list of processed files\n",
    "PERSIST_DIR = \"./data/chroma_db\"            #embeddings database (store vectorized chunks)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e1787",
   "metadata": {},
   "source": [
    "#### 1.2. Check if the input is already processed or is a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files():\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_new_files():\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        print(f\"Folder {FOLDER_PATH} does not exist.\")\n",
    "        return []\n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dc0ae",
   "metadata": {},
   "source": [
    "### 2. Split document into chunks of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba878d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & chunk helpers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords)\n",
    "        or any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate chunks\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    chunks, current_chunk, current_tokens = [], [], 0\n",
    "    metadata = []\n",
    "\n",
    "    for block in raw_blocks:\n",
    "        sentences = nltk.sent_tokenize(block.strip())\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)\n",
    "\n",
    "            if is_code_like(sentence):\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0\n",
    "\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += tokens\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    if inspect:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f0a2a",
   "metadata": {},
   "source": [
    "### 3. Embed all chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d7f6d",
   "metadata": {},
   "source": [
    "#### 3.1. Initialize sentence-transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274a8cc",
   "metadata": {},
   "source": [
    "#### 3.2. Save embeddings into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreManager:\n",
    "    def __init__(self, persist_dir, embeddings):\n",
    "        self.persist_dir = persist_dir\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstore = self._load_existing()\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Attempts to load an existing Chroma DB from the persist directory.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"  # Add this line\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing Chroma DB: {e}\")\n",
    "            print(\"Will create a new one.\")\n",
    "        \n",
    "        print(\"\\n-> No existing Chroma DB found. A new one will be created when chunks are added.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Adds chunks to the vector store, creating it if it doesn't exist.\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"No chunks to add.\")\n",
    "            return\n",
    "\n",
    "        if not self.vectorstore:\n",
    "            print(f\"\\n-> Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,  # Changed from embedding_function\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"   # Add this line\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "    def get_chunks_by_file(self, fname):\n",
    "        \"\"\"Retrieves all chunks associated with a specific source file.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vectorstore is not initialized. Cannot retrieve chunks.\")\n",
    "            return []\n",
    "        results = self.vectorstore.get(where={\"source\": fname})\n",
    "        return results.get('documents', []) # Safely get documents\n",
    "\n",
    "    def get_all_chunks(self):\n",
    "        \"\"\"Retrieves all chunks from the vector store.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vectorstore is not initialized. Cannot retrieve chunks.\")\n",
    "            return []\n",
    "        results = self.vectorstore.get(include=[\"documents\"])\n",
    "        return results.get('documents', [])\n",
    "    \n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieves chunks and their embeddings for a specific file.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vectorstore is not initialized. Cannot retrieve chunks.\")\n",
    "            return [], []\n",
    "        \n",
    "        try:\n",
    "            results = self.vectorstore.get(\n",
    "                where={\"source\": fname},\n",
    "                include=[\"documents\", \"embeddings\"]\n",
    "            )\n",
    "            chunks = results.get('documents', [])\n",
    "            embeddings_list = results.get('embeddings', [])\n",
    "            \n",
    "            if not chunks:\n",
    "                print(f\"No chunks found for file: {fname}\")\n",
    "                return [], []\n",
    "            \n",
    "            if embeddings_list is None or len(embeddings_list) == 0:\n",
    "                print(f\"No embeddings found for file: {fname}. Computing embeddings...\")\n",
    "                # Fallback: compute embeddings manually if not stored\n",
    "                embeddings_list = [self.embeddings.embed_query(chunk) for chunk in chunks]\n",
    "            \n",
    "            return chunks, embeddings_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving chunks with embeddings for file {fname}: {e}\")\n",
    "            return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669203d8",
   "metadata": {},
   "source": [
    "### 4. Cluster the embeddings by semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings):\n",
    "        self.chunks = chunks                                                        # Store text chunks for processing\n",
    "        self.embeddings = np.array(chunk_embeddings) if chunk_embeddings else None  # Convert embeddings to numpy array for ML operations\n",
    "        self.labels = []                                                            # Initialize empty list to store cluster labels\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        \n",
    "        if not self.chunks:                                                    # Check if chunks list is empty\n",
    "            print(\"No chunks to cluster.\")\n",
    "            return []\n",
    "            \n",
    "        if len(self.chunks) < min_cluster_size:                                # Check if we have enough chunks to form clusters\n",
    "            print(f\"Not enough chunks to cluster. Need at least {min_cluster_size}, but have {len(self.chunks)}.\")\n",
    "            self.labels = [-1] * len(self.chunks)                              # Assign all chunks to noise cluster (-1)\n",
    "            return self.labels\n",
    "\n",
    "        if self.embeddings is None or self.embeddings.size == 0:              # Check if embeddings exist and are not empty\n",
    "            print(\"No embeddings available for clustering.\")\n",
    "            self.labels = [-1] * len(self.chunks)                             # Assign all chunks to noise cluster (-1)\n",
    "            return self.labels\n",
    "\n",
    "        try:\n",
    "            if self.embeddings.ndim == 1:                                     # Check if embeddings are 1-dimensional\n",
    "                self.embeddings = self.embeddings.reshape(1, -1)              # Reshape to 2D: (1, n_features)\n",
    "            \n",
    "            print(f\"Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "            clusterizer = hdbscan.HDBSCAN(                                     # Initialize HDBSCAN clustering algorithm\n",
    "                min_cluster_size=min_cluster_size,                             # Minimum number of points in a cluster\n",
    "                min_samples=min_samples,                                       # Minimum samples in neighborhood for core point\n",
    "                metric='euclidean'                                             # Distance metric for clustering\n",
    "            )\n",
    "            self.labels = clusterizer.fit_predict(self.embeddings)             # Fit model and predict cluster labels\n",
    "\n",
    "            num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)  # Count unique clusters (excluding noise)\n",
    "            print(f\"\\nFound {num_clusters} clusters in the document.\")\n",
    "            print(f\"Cluster distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")  # Show cluster sizes\n",
    "            \n",
    "            for cluster_id in sorted(set(self.labels)):                       # Iterate through each unique cluster ID\n",
    "                cluster_name = \"Noise (ungrouped chunks)\" if cluster_id == -1 else f\"Cluster {cluster_id}\"  # Name clusters\n",
    "                chunk_count = sum(1 for label in self.labels if label == cluster_id)  # Count chunks in this cluster\n",
    "                print(f\"\\n--- {cluster_name} ({chunk_count} chunks) ---\")\n",
    "                \n",
    "                for i, label in enumerate(self.labels):                       # Iterate through all chunk labels\n",
    "                    if label == cluster_id:                                   # Check if chunk belongs to current cluster\n",
    "                        preview = self.chunks[i][:200] + ('...' if len(self.chunks[i]) > 200 else '')  # Create preview text\n",
    "                        print(f\"Chunk {i+1}: {preview}\\n\")                    # Print chunk preview\n",
    "            \n",
    "            return self.labels                                                # Return cluster labels\n",
    "            \n",
    "        except Exception as e:                                                # Handle any clustering errors\n",
    "            print(f\"Error during clustering: {e}\")\n",
    "            self.labels = [-1] * len(self.chunks)                             # Assign all chunks to noise on error\n",
    "            return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21902a3c",
   "metadata": {},
   "source": [
    "### Entry point to run steps 1 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname):\n",
    "    if fname in get_processed_files():\n",
    "        print(f\"Skipping {fname} (already processed).\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing new file: {fname}\")\n",
    "    # 1. Read text\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 2. Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)\n",
    "\n",
    "    # 3. Initialize vector manager\n",
    "    vector_manager = VectorStoreManager(PERSIST_DIR, embeddings)\n",
    "    vector_manager.add_chunks(chunks, metadata)\n",
    "\n",
    "    # 4. Mark file as processed \n",
    "    with open(LOG_PATH, \"a\") as log:\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    print(f\"\\nAdded {len(chunks)} chunks from {fname} to Chroma DB.\")\n",
    "\n",
    "    # 5. Cluster chunks from new file\n",
    "    print(f\"\\n--- Starting Clustering Process for '{fname}' ---\")\n",
    "\n",
    "    # Retrieve the chunks AND embeddings for the file just processed\n",
    "    new_chunks, new_embeddings = vector_manager.get_chunks_with_embeddings(fname)\n",
    "\n",
    "    \n",
    "    if (new_chunks and                                                  # Check if array exists and has elements\n",
    "    new_embeddings is not None and \n",
    "    len(new_embeddings) > 0 and \n",
    "    any(len(emb) > 0 for emb in new_embeddings)):\n",
    "        \n",
    "        clusterizer = ChunkClusterizer(new_chunks, new_embeddings)      # Pass the chunks and their pre-computed embeddings to the clusterizer\n",
    "        clusterizer.cluster_chunks()\n",
    "    else:\n",
    "        print(f\"Could not retrieve chunks for {fname} to cluster.\")    \n",
    "    \n",
    "\n",
    "\n",
    "def process_all_new():\n",
    "    new_files = get_new_files()\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files to process: {new_files}\")\n",
    "    \n",
    "    for f in new_files:\n",
    "        process_file(f)\n",
    "    print(\"\\nAll new files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506921b",
   "metadata": {},
   "source": [
    "### Option 1: Process all new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 new files to process: ['note-2-ul.txt']\n",
      "\n",
      "Processing new file: note-2-ul.txt\n",
      "\n",
      "Chunk 1 (7 tokens):\n",
      "unsup = no labels.\n",
      "\n",
      "Chunk 2 (12 tokens):\n",
      "kmeans = simplest one but still used a lot.\n",
      "\n",
      "Chunk 3 (59 tokens):\n",
      "init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat. converge when no pt changes. but result depends on init + scale. scale important! feature w bigger range dominates dist calc — always standardize first.\n",
      "\n",
      "Chunk 4 (19 tokens):\n",
      "elbow method = plot inertia vs k — look for bend, but not always obvious.\n",
      "\n",
      "Chunk 5 (13 tokens):\n",
      "inertia = sum of dist² to centroid.\n",
      "\n",
      "Chunk 6 (20 tokens):\n",
      "alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered.\n",
      "\n",
      "Chunk 7 (73 tokens):\n",
      "clustering ≠ classification. labels are not known. use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples. tricky to tune tho. forms clusters based on density, noisy pts marked as outliers (label -1).\n",
      "\n",
      "Chunk 8 (65 tokens):\n",
      "sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN(eps=0.5, min_samples=5) model.fit(X)\n",
      "hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative).\n",
      "\n",
      "Chunk 9 (16 tokens):\n",
      "start w all pts as indiv cluster, merge closest pairs step by step.\n",
      "\n",
      "Chunk 10 (9 tokens):\n",
      "dendrogram = tree of merges.\n",
      "\n",
      "Chunk 11 (16 tokens):\n",
      "can \"cut\" tree at diff levels = diff num clusters.\n",
      "\n",
      "Chunk 12 (38 tokens):\n",
      "linkage: single, complete, avg. sklearn has AgglomerativeClustering. before clustering, can reduce dim (PCA) for speed + viz.\n",
      "\n",
      "Chunk 13 (18 tokens):\n",
      "tSNE/UMAP for 2D plot = better for human eye but not for modeling.\n",
      "\n",
      "Chunk 14 (12 tokens):\n",
      "PCA = linear, tSNE = non-linear.\n",
      "\n",
      "Chunk 15 (13 tokens):\n",
      "tSNE distorts structure globally. good for pattern discovery.\n",
      "\n",
      "Chunk 16 (28 tokens):\n",
      "spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace.\n",
      "\n",
      "Chunk 17 (92 tokens):\n",
      "nice when structure is graphy, not spherical. most clustering algos rely on distance metric — Euclidean default. alt: cosine sim (for text), manhattan, etc. eval: hard bcz no true label. silhouette best for most. DB index too. compare within/between cluster distance. can also visualize clusters to judge quality. problem: k-means assumes spherical clusters, equal size. not true for real-world.\n",
      "\n",
      "Chunk 18 (11 tokens):\n",
      "if data has diff density or shapes → fails.\n",
      "\n",
      "Chunk 19 (31 tokens):\n",
      "open Q: how to know if clusters mean anything in real world? scaling is essential — StandardScaler or MinMaxScaler from sklearn.\n",
      "\n",
      "Chunk 20 (42 tokens):\n",
      "pipeline ex:\n",
      "from sklearn.pipeline import make_pipeline pipe = make_pipeline(StandardScaler(), KMeans(n_clusters=3)) pipe.fit(X)\n",
      "Loading existing Chroma DB from ./data/chroma_db\n",
      "\n",
      "Added 20 chunks from note-2-ul.txt to Chroma DB.\n",
      "\n",
      "--- Starting Clustering Process for 'note-2-ul.txt' ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_all_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mprocess_all_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new files to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m new_files:\n",
      "\u001b[0;32m---> 52\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll new files processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(fname)\u001b[0m\n",
      "\u001b[1;32m     28\u001b[0m new_chunks, new_embeddings \u001b[38;5;241m=\u001b[39m vector_manager\u001b[38;5;241m.\u001b[39mget_chunks_with_embeddings(fname)\n",
      "\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (new_chunks \u001b[38;5;129;01mand\u001b[39;00m                                                  \u001b[38;5;66;03m# Check if array exists and has elements\u001b[39;00m\n",
      "\u001b[1;32m     32\u001b[0m new_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \n",
      "\u001b[1;32m     33\u001b[0m \u001b[38;5;28mlen\u001b[39m(new_embeddings) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \n",
      "\u001b[1;32m     34\u001b[0m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emb) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m new_embeddings)):\n",
      "\u001b[0;32m---> 36\u001b[0m     clusterizer \u001b[38;5;241m=\u001b[39m \u001b[43mChunkClusterizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_embeddings\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# Pass the chunks and their pre-computed embeddings to the clusterizer\u001b[39;00m\n",
      "\u001b[1;32m     37\u001b[0m     clusterizer\u001b[38;5;241m.\u001b[39mcluster_chunks()\n",
      "\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mChunkClusterizer.__init__\u001b[0;34m(self, chunks, chunk_embeddings)\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, chunks, chunk_embeddings):\n",
      "\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;241m=\u001b[39m chunks                                                        \u001b[38;5;66;03m# Store text chunks for processing\u001b[39;00m\n",
      "\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(chunk_embeddings) \u001b[38;5;28;01mif\u001b[39;00m chunk_embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Convert embeddings to numpy array for ML operations\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "process_all_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e92616",
   "metadata": {},
   "source": [
    "### Option 2: Process 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\"note-1-dt-r.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205dabf0",
   "metadata": {},
   "source": [
    "## Continue pipeline...\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Label clusters with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Rewrite selected outputs with [llama 3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    7. Store each sub-document in correspondent label as a new file\n",
    "#      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.    8. Add more documents in bulk and generate insights for simple EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7994463",
   "metadata": {},
   "source": [
    "# clean code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### IMPORT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ### CONFIGURATION\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          # Input files directory\n",
    "LOG_PATH = \"./data/processed_files.txt\"     # Processed files log\n",
    "PERSIST_DIR = \"./data/chroma_db\"            # Vector database directory\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ### UTILITY FUNCTIONS\n",
    "def get_processed_files():\n",
    "    \"\"\"Load list of already processed files from log.\"\"\"\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "def get_new_files():\n",
    "    \"\"\"Get list of new .txt files that haven't been processed yet.\"\"\"\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        return []\n",
    "    \n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using the embedding model's tokenizer.\"\"\"\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    \"\"\"Detect if a sentence contains code-like patterns.\"\"\"\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords) or\n",
    "        any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "# ### CHUNKING LOGIC\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    \"\"\"Split text into semantic chunks with token limits.\"\"\"\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())                    # Split by double newlines\n",
    "    chunks, current_chunk, current_tokens = [], [], 0                  # Initialize chunking variables\n",
    "    metadata = []                                                      # Store metadata for each chunk\n",
    "\n",
    "    for block in raw_blocks:                                           # Process each text block\n",
    "        sentences = nltk.sent_tokenize(block.strip())                  # Split block into sentences\n",
    "\n",
    "        for sentence in sentences:                                     # Process each sentence\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)                            # Count tokens in sentence\n",
    "\n",
    "            # Handle code-like sentences separately\n",
    "            if is_code_like(sentence):                                 # Check if sentence contains code\n",
    "                if current_chunk:                                      # Save current chunk if exists\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)                                # Add code as separate chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            # Handle conversation transitions\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:                                      # Break chunk at transition words\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            # Check if adding sentence would exceed max tokens\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))                # Save current chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0                 # Reset for new chunk\n",
    "\n",
    "            current_chunk.append(sentence)                            # Add sentence to current chunk\n",
    "            current_tokens += tokens                                  # Update token count\n",
    "\n",
    "        # Save any remaining chunk after block processing\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    if inspect:                                                       # Optional: print chunks for debugging\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata\n",
    "\n",
    "# ### VECTOR STORE MANAGEMENT\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, persist_dir, embeddings):\n",
    "        self.persist_dir = persist_dir                                # Store database directory path\n",
    "        self.embeddings = embeddings                                  # Store embedding model\n",
    "        self.vectorstore = self._load_existing()                     # Load existing DB or set to None\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Load existing Chroma database if it exists.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing Chroma DB: {e}\")\n",
    "        \n",
    "        print(\"No existing Chroma DB found. Will create new one.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Add text chunks to vector store with embeddings.\"\"\"\n",
    "        if not self.vectorstore:                                     # Create new database if none exists\n",
    "            print(f\"Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"\n",
    "            )\n",
    "        else:                                                        # Add to existing database\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieve chunks and embeddings for a specific file.\"\"\"\n",
    "        results = self.vectorstore.get(                              # Query database for file chunks\n",
    "            where={\"source\": fname},\n",
    "            include=[\"documents\", \"embeddings\"]\n",
    "        )\n",
    "        \n",
    "        chunks = results.get('documents', [])                        # Extract text chunks\n",
    "        embeddings_list = results.get('embeddings', [])             # Extract embeddings\n",
    "        \n",
    "        # Fallback: compute embeddings if not stored\n",
    "        if not embeddings_list:                                      # Generate embeddings if missing\n",
    "            print(f\"Computing embeddings for {fname}...\")\n",
    "            embeddings_list = [self.embeddings.embed_query(chunk) for chunk in chunks]\n",
    "        \n",
    "        return chunks, embeddings_list\n",
    "\n",
    "# ### CLUSTERING LOGIC\n",
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings):\n",
    "        self.chunks = chunks                                         # Store text chunks\n",
    "        self.embeddings = np.array(chunk_embeddings)                # Convert embeddings to numpy array\n",
    "        self.labels = []                                             # Initialize cluster labels\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        \"\"\"Cluster chunks using HDBSCAN algorithm.\"\"\"\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if len(self.chunks) < min_cluster_size:                      # Not enough chunks to cluster\n",
    "            print(f\"Not enough chunks to cluster ({len(self.chunks)} < {min_cluster_size})\")\n",
    "            self.labels = [-1] * len(self.chunks)                   # Assign all to noise\n",
    "            return self.labels\n",
    "\n",
    "        # Ensure embeddings are 2D\n",
    "        if self.embeddings.ndim == 1:                               # Reshape 1D embeddings to 2D\n",
    "            self.embeddings = self.embeddings.reshape(1, -1)\n",
    "\n",
    "        print(f\"Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "        \n",
    "        # Perform clustering\n",
    "        clusterizer = hdbscan.HDBSCAN(                              # Initialize clustering algorithm\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        self.labels = clusterizer.fit_predict(self.embeddings)      # Generate cluster labels\n",
    "\n",
    "        # Display results\n",
    "        num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)  # Count clusters (exclude noise)\n",
    "        print(f\"Found {num_clusters} clusters\")\n",
    "        print(f\"Cluster distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "\n",
    "        # Show cluster contents\n",
    "        for cluster_id in sorted(set(self.labels)):                 # Iterate through each cluster\n",
    "            cluster_name = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "            chunk_count = sum(1 for label in self.labels if label == cluster_id)\n",
    "            print(f\"\\n--- {cluster_name} ({chunk_count} chunks) ---\")\n",
    "            \n",
    "            for i, label in enumerate(self.labels):                 # Show chunks in this cluster\n",
    "                if label == cluster_id:\n",
    "                    preview = self.chunks[i][:200] + ('...' if len(self.chunks[i]) > 200 else '')\n",
    "                    print(f\"Chunk {i+1}: {preview}\\n\")\n",
    "\n",
    "        return self.labels\n",
    "\n",
    "# ### MAIN PROCESSING PIPELINE\n",
    "def process_file(fname):\n",
    "    \"\"\"Process a single file through the complete pipeline.\"\"\"\n",
    "    print(f\"\\n=== Processing: {fname} ===\")\n",
    "    \n",
    "    # Step 1: Read file\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Step 2: Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)        # Split text into chunks\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "    # Step 3: Store in vector database\n",
    "    vector_manager = VectorStoreManager(PERSIST_DIR, embeddings)    # Initialize vector store\n",
    "    vector_manager.add_chunks(chunks, metadata)                     # Add chunks with embeddings\n",
    "    print(f\"Stored {len(chunks)} chunks in vector database\")\n",
    "\n",
    "    # Step 4: Mark as processed\n",
    "    with open(LOG_PATH, \"a\") as log:                                # Log processed file\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    # Step 5: Cluster chunks\n",
    "    print(f\"\\n--- Clustering chunks from {fname} ---\")\n",
    "    new_chunks, new_embeddings = vector_manager.get_chunks_with_embeddings(fname)  # Retrieve stored data\n",
    "    \n",
    "    clusterizer = ChunkClusterizer(new_chunks, new_embeddings)      # Initialize clustering\n",
    "    clusterizer.cluster_chunks()                                    # Perform clustering\n",
    "\n",
    "def process_all_new():\n",
    "    \"\"\"Process all new files in the input directory.\"\"\"\n",
    "    new_files = get_new_files()                                     # Get list of unprocessed files\n",
    "    \n",
    "    if not new_files:                                               # Exit early if no new files\n",
    "        print(\"No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files: {new_files}\")\n",
    "    \n",
    "    for fname in new_files:                                         # Process each new file\n",
    "        process_file(fname)\n",
    "    \n",
    "    print(f\"\\n=== Completed processing {len(new_files)} files ===\")\n",
    "\n",
    "# ### EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_new()                                               # Run the complete pipeline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
