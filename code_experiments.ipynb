{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1cbf21",
   "metadata": {},
   "source": [
    "# clean the Chroma DB:\n",
    "-  on terminal:\n",
    "rm -rf ./data/chroma_db\n",
    "- delete list from \"processed_files.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d34171",
   "metadata": {},
   "source": [
    "# First experiment: Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(model: str, prompt: str) -> str:\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt.encode('utf-8'),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "    return result.stdout.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e2131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here's the organized and rewritten version of your text according to your instructions:\n",
      "\n",
      "[Idea]\n",
      "- I had an idea for a new project.\n",
      "\n",
      "[Task]\n",
      "- Later I thought maybe I should buy cat food.\n",
      "\n",
      "[Message]\n",
      "- [Idea: Sharing my idea about a new project]\n",
      "- [Task: Suggesting that Alex might help with buying cat food]\n",
      "\n",
      "Rewritten and Reordered version:\n",
      "\n",
      "1. First, I'd like to share an idea I had for a new project - do you think you could help discuss it?\n",
      "2. Later, I thought it would be good if we could buy some cat food together as I need to take care of my pet. Let me know what you think.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I had an idea for a new project. Also I need to buy cat food. \n",
    "Later I thought maybe I should send that message to Alex.\n",
    "\n",
    "1. Group related sentences.\n",
    "2. Label each group as: [Idea, Task, Message]\n",
    "3. Reorder and rewrite each clearly.\n",
    "\"\"\"\n",
    "\n",
    "response = query_ollama(\"mistral\", prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c360c",
   "metadata": {},
   "source": [
    "It didn't work exactly how I expected and it took too long to give me an answer. Prompt engineering refinement, but still, hard to test. Changing approach to classical NLP and using generative AI models only for rewriting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08bdc6",
   "metadata": {},
   "source": [
    "# word tokenize + get chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe496622",
   "metadata": {},
   "source": [
    "- Analysis: The [word_toneknize] splits the punctiation into separate tokens and creates this view that makes harder to compreehend the context. I researched other tokenizer and found a \"de-tokenizer\" [tree bank word] to join the tokens in a more natural way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to split documents with Word+Tokenize ###\n",
    "\n",
    "def split_by_words(text, chunk_size=80, overlap=20):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "# 2. Split only new files into chunks of words\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "else:\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    for fname in new_files:\n",
    "        with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            display(f\"Text file: {fname}\", Markdown(f\"```\\n{text}\\n```\"))\n",
    "        chunks = split_by_words(text)\n",
    "        all_chunks.extend(chunks)\n",
    "        all_metadatas.extend([{\"source\": fname}] * len(chunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33295bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 chunks from note-1-dt-r.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 ( p ) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster ? less bias ? not 100 % sure . trees go deep and then prone back ? no , prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right ? sklearn.tree.DecisionTreeClassifier ( max_depth=3 ) — yeah that ’ s what prof used . note : good to visualize trees but not for high dim data . lots of axis-aligned splits , hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum ( w² ) , shrinks weights , but all stay ≠ 0 . Lasso adds λ * sum ( |w| ) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board : from sklearn.linear_model import Lasso model = Lasso ( alpha=0.1 ) model.fit ( X_train , y_train ) ElasticNet = mix of both ? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting .\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible model class I think ?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 ( p ) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster ? less bias ? not 100 % sure . trees go deep and then prone back ? no , prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right ? sklearn.tree.DecisionTreeClassifier ( max_depth=3 ) — yeah that ’ s what prof used . note : good to visualize trees but not for high dim data . lots of axis-aligned splits , hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum ( w² ) , shrinks weights , but all stay ≠ 0 . Lasso adds λ * sum ( |w| ) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board : from sklearn.linear_model import Lasso model = Lasso ( alpha=0.1 ) model.fit ( X_train , y_train ) ElasticNet = mix of both ? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting .\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like , not smooth like linear models . ok Q : why trees overfit more than lasso ? more flexible model class I think ?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### get the chunks from a specific file\n",
    "\n",
    "def get_file_chunks(fname):\n",
    "    results = vectorstore.get(where={\"source\": fname})\n",
    "    file_chunks = results['documents']\n",
    "    print(f\"Found {len(file_chunks)} chunks from {fname}\")\n",
    "    for chunk in file_chunks:\n",
    "        display(Markdown(f\"```\\n{chunk}\\n```\"))\n",
    "\n",
    "file1_chunks = get_file_chunks(\"note-1-dt-r.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0aca9",
   "metadata": {},
   "source": [
    "### 2. Document segmentation (text split)\n",
    "Split the document into chunks of 80 words, which is enough to capture the semantic context. Sentences and words would miss the context. Paragraphs are not consistent with unstructured documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to split document with TreebankWordDetokenizer ###\n",
    "\n",
    "def split_by_words_de(text, chunk_size=80, overlap=20):\n",
    "    words = word_tokenize(text)\n",
    "    detok = TreebankWordDetokenizer()                       # detokenizer to join the tokens in a more natural way\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(detok.detokenize(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a554f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original file: note-3-dt-r-ul.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "lecture today was fast af. started with trees. entropy vs gini impurity — diff metrics to decide best split. both OK. CART = binary tree = each node has 2 splits. sklearn uses this. code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
       "trees prone to overfit — esp if depth unbounded. pruning = way to fix. early stopping or post-prune. bagging helps = ensemble. RandomForest = multiple trees on bootstrapped samples + rand subset of features per split. reduces variance.\n",
       "trees = interpretability good, but unstable to data changes.\n",
       "then prof jumped into regression. regularization = shrink model capacity. Ridge = L2 norm = λ * Σ(w²). Lasso = L1 norm = λ * Σ|w|. Ridge keeps all weights ≠ 0, Lasso can zero out → sparse. Lasso good for feature selection. ElasticNet = mix of both — good if features correlated\n",
       "code ex:\n",
       "from sklearn.linear_model import ElasticNet model = ElasticNet(alpha=0.1, l1_ratio=0.5) model.fit(X, y)\n",
       "important: scale features before fitting regularized models — otherwise magnitudes skew the penalty. StandardScaler or RobustScaler if outliers.\n",
       "tune α via cross-val — use GridSearchCV or RandomizedSearchCV.\n",
       "metrics: RMSE, R². underfit vs overfit — regularization helps balance bias/var tradeoff.\n",
       "last part was unsupervised learning. clustering w/o labels. k-means = most used. init centers, assign pts, recalc, repeat. problem: sensitive to init. use k-means++. elbow method not always clear. silhouette score better maybe.\n",
       "PCA + k-means often combined for vis. t-SNE only for viz — not for modeling. clusters in t-SNE are sometimes fake.\n",
       "DBSCAN = cluster via density. can detect noise. great for shape-agnostic clusters. params hard to tune tho. hierarchical = dendrograms. use ‘ward’ linkage. but slow w/ big data.\n",
       "general note: sklearn models consistent API — fit / predict / score.\n",
       "also: why trees don’t need scaling? bcz splits based on order not value. contrast w/ reg models.\n",
       "each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note-3-dt-r-ul.txt = 7 chunks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "lecture today was fast af . started with trees . entropy vs gini impurity — diff metrics to decide best split . both OK. CART = binary tree = each node has 2 splits . sklearn uses this . code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier (criterion='entropy', max_depth=4) trees prone to overfit — esp if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       "if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble . RandomForest = multiple trees on bootstrapped samples + rand subset of features per split . reduces variance . trees = interpretability good, but unstable to data changes . then prof jumped into regression . regularization = shrink model capacity . Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ *)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       ". Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ * Σ|w| . Ridge keeps all weights ≠ 0, Lasso can zero out → sparse . Lasso good for feature selection . ElasticNet = mix of both — good if features correlated code ex: from sklearn.linear_model import ElasticNet model = ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:**\n",
       "= ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized models — otherwise magnitudes skew the penalty . StandardScaler or RobustScaler if outliers . tune α via cross-val — use GridSearchCV or RandomizedSearchCV . metrics: RMSE, R² . underfit vs overfit — regularization helps balance bias/var tradeoff . last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:**\n",
       "last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts, recalc, repeat . problem: sensitive to init . use k-means++ . elbow method not always clear . silhouette score better maybe . PCA + k-means often combined for vis . t-SNE only for viz — not for modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:**\n",
       "modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great for shape-agnostic clusters . params hard to tune tho . hierarchical = dendrograms . use ‘ ward ’ linkage . but slow w/ big data . general note: sklearn models consistent API — fit / predict / score . also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 7:**\n",
       "also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/ reg models . each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Original file: note-1-dt-r.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2(p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node. Gini impurity also similar but faster? less bias? not 100% sure. trees go deep and then prone back? no, prune. to prevent overfitting. training error low but generalization bad. CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier(max_depth=3) — yeah that’s what prof used.\n",
       "note: good to visualize trees but not for high dim data. lots of axis-aligned splits, hard to interpret when too many features. oh and trees are unstable — small data change = big model change. they said bagging can help that — RandomForest.\n",
       "wait then they jumped to regularization — lasso vs ridge. ridge adds λ * sum(w²), shrinks weights, but all stay ≠ 0. Lasso adds λ * sum(|w|) — forces some to zero. ohhh good for feature selection. balance bias-variance tradeoff. λ too big = underfit. low λ = overfit. prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso(alpha=0.1) model.fit(X_train, y_train)\n",
       "ElasticNet = mix of both? ratio param controls mix. good when multicollinearity or many small coeffs. visualize loss function — lasso diamond corners cause zeros. interesting.\n",
       "btw they said don’t scale trees but do scale for lasso etc. bcz regularization depends on magnitude. std scaling or minmax okay.\n",
       "might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV\n",
       "decision boundary of tree is step-like, not smooth like linear models.\n",
       "ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note-1-dt-r.txt = 6 chunks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:**\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:**\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:**\n",
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Original file: note-2-ul.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "unsup = no labels. kmeans = simplest one but still used a lot. init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat. converge when no pt changes. but result depends on init + scale. scale important! feature w bigger range dominates dist calc — always standardize first.\n",
       "elbow method = plot inertia vs k — look for bend, but not always obvious. inertia = sum of dist² to centroid. alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered.\n",
       "clustering ≠ classification. labels are not known. use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples. tricky to tune tho. forms clusters based on density, noisy pts marked as outliers (label -1). sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN(eps=0.5, min_samples=5) model.fit(X)\n",
       "hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w all pts as indiv cluster, merge closest pairs step by step. dendrogram = tree of merges. can \"cut\" tree at diff levels = diff num clusters. linkage: single, complete, avg. sklearn has AgglomerativeClustering.\n",
       "before clustering, can reduce dim (PCA) for speed + viz. tSNE/UMAP for 2D plot = better for human eye but not for modeling. PCA = linear, tSNE = non-linear. tSNE distorts structure globally. good for pattern discovery.\n",
       "spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace. nice when structure is graphy, not spherical.\n",
       "most clustering algos rely on distance metric — Euclidean default. alt: cosine sim (for text), manhattan, etc.\n",
       "eval: hard bcz no true label. silhouette best for most. DB index too. compare within/between cluster distance. can also visualize clusters to judge quality.\n",
       "problem: k-means assumes spherical clusters, equal size. not true for real-world. if data has diff density or shapes → fails.\n",
       "open Q: how to know if clusters mean anything in real world?\n",
       "scaling is essential — StandardScaler or MinMaxScaler from sklearn.\n",
       "pipeline ex:\n",
       "from sklearn.pipeline import make_pipeline pipe = make_pipeline(StandardScaler(), KMeans(n_clusters=3)) pipe.fit(X)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note-2-ul.txt = 8 chunks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "unsup = no labels . kmeans = simplest one but still used a lot . init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat . converge when no pt changes . but result depends on init + scale . scale important! feature w bigger range dominates dist calc — always standardize first . elbow method = plot inertia vs k — look for bend, but not always obvious . inertia =)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       ". elbow method = plot inertia vs k — look for bend, but not always obvious . inertia = sum of dist² to centroid . alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered . clustering ≠ classification . labels are not known . use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps +)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       ", anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples . tricky to tune tho . forms clusters based on density, noisy pts marked as outliers (label -1). sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN (eps=0.5, min_samples=5) model.fit (X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:**\n",
       "(X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w all pts as indiv cluster, merge closest pairs step by step . dendrogram = tree of merges . can \"cut\" tree at diff levels = diff num clusters . linkage: single, complete, avg . sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:**\n",
       "sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for 2D plot = better for human eye but not for modeling . PCA = linear, tSNE = non-linear . tSNE distorts structure globally . good for pattern discovery . spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:**\n",
       "→ k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on distance metric — Euclidean default . alt: cosine sim (for text), manhattan, etc . eval: hard bcz no true label . silhouette best for most . DB index too . compare within/between cluster distance . can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 7:**\n",
       ". can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not true for real-world . if data has diff density or shapes → fails . open Q: how to know if clusters mean anything in real world? scaling is essential — StandardScaler or MinMaxScaler from sklearn . pipeline ex: from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit ()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 8:**\n",
       ": from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit (X))\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Split document into chunks of words\n",
    "\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "else:\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    for fname in new_files:\n",
    "        with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    \n",
    "        chunks = split_by_words_de(text)                                    #split in chunks\n",
    "        display(f\"Original file: {fname}\", Markdown(f\"```\\n{text}\\n```\"))   #display original file(s)\n",
    "        print(f\"{fname} = {len(chunks)} chunks\")                            #display number of chunks per file\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            display(Markdown(f\"**Chunk {i}:**\\n{chunk})\\n\"))\n",
    "        all_chunks.extend(chunks)\n",
    "        all_metadatas.extend([{\"source\": fname}] * len(chunks))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f4920",
   "metadata": {},
   "source": [
    "### 3. Embedding new chunks from new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/9xl167095gn7qzk16djnlv300000gp/T/ipykernel_34806/884519275.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Chroma DB in: ./data/chroma_db\n",
      "Processed and added 3 new files to Chroma DB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/9xl167095gn7qzk16djnlv300000gp/T/ipykernel_34806/884519275.py:23: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# 3.1. Embed new chunks\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# 3.2. Add embeddings to Chroma DB\n",
    "persist_dir = \"./data/chroma_db\"\n",
    "\n",
    "if os.path.exists(persist_dir):\n",
    "    print(f\"Loading existing Chroma DB from: {persist_dir}\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    vectorstore.add_texts(texts=all_chunks, metadatas=all_metadatas)\n",
    "else:\n",
    "    print(f\"Creating new Chroma DB in: {persist_dir}\")\n",
    "    vectorstore = Chroma.from_texts(\n",
    "        texts=all_chunks, \n",
    "        metadatas=all_metadatas, \n",
    "        embedding=embeddings, \n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "vectorstore.persist()\n",
    "\n",
    "\n",
    "# 3.3. Update processed files log\n",
    "with open(log_path, \"a\") as f:\n",
    "    for fname in new_files:\n",
    "        f.write(fname + \"\\n\")\n",
    "print(f\"Processed and added {len(new_files)} new files to Chroma DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4fa6a",
   "metadata": {},
   "source": [
    "### 4. Clusterize by semantic similarity\n",
    "1 exprimente: HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications with Noise -> not good. moved to other file\n",
    "2 experiment: kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to retrieve the chunks from a specific file\n",
    "\n",
    "def get_file_chunks(fname):\n",
    "    results = vectorstore.get(where={\"source\": fname})\n",
    "    file_chunks = results['documents']\n",
    "    print(f\"Found {len(file_chunks)} chunks from {fname}\")\n",
    "    for chunk in file_chunks:\n",
    "        display(Markdown(f\"```\\n{chunk}\\n```\"))             # display the contents of each chunk for conference\n",
    "    return file_chunks                                      # return the list for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 chunks from note-1-dt-r.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.1. Get embeddings chunks from File 1\n",
    "\n",
    "file1_chunks = get_file_chunks(\"note-1-dt-r.txt\")\n",
    "file1_embeddings = embeddings.embed_documents(file1_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans cluster labels: [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 2  # or 1, 2, 3 depending on your expectation\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(file1_embeddings)\n",
    "print(\"KMeans cluster labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b7bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cluster 0 ---\n",
      "Chunk 1:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 2:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 3:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big ="
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Cluster 1 ---\n",
      "Chunk 4:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 5:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 6:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Function to display chunks in clusters  ###\n",
    "\n",
    "for cluster_id in set(labels):\n",
    "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == cluster_id:\n",
    "            print(f\"Chunk {i+1}:\")\n",
    "            #print(file1_chunks[i]) \n",
    "            display(Markdown(file1_chunks[i]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7802adc",
   "metadata": {},
   "source": [
    "chat gpt splitting: \n",
    "1. entropy = -p * log2(p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node. Gini impurity also similar but faster? less bias? not 100% sure. trees go deep and then prone back? no, prune. to prevent overfitting. training error low but generalization bad. CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier(max_depth=3) — yeah that’s what prof used.\n",
    "note: good to visualize trees but not for high dim data. lots of axis-aligned splits, hard to interpret when too many features. oh and trees are unstable — small data change = big model change. they said bagging can help that — RandomForest.\n",
    "decision boundary of tree is step-like, not smooth like linear models.\n",
    "ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
    "btw they said don’t scale trees\n",
    "\n",
    "2. wait then they jumped to regularization — lasso vs ridge. ridge adds λ * sum(w²), shrinks weights, but all stay ≠ 0. Lasso adds λ * sum(|w|) — forces some to zero. ohhh good for feature selection. balance bias-variance tradeoff. λ too big = underfit. low λ = overfit. prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso(alpha=0.1) model.fit(X_train, y_train)\n",
    "ElasticNet = mix of both? ratio param controls mix. good when multicollinearity or many small coeffs. visualize loss function — lasso diamond corners cause zeros. interesting.\n",
    "btw they said […] do scale for lasso etc. bcz regularization depends on magnitude. std scaling or minmax okay.\n",
    "might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.54851236 0.37322338 0.11788947 0.25886513 0.30239811]\n",
      " [0.54851236 1.         0.44631097 0.33833166 0.44378826 0.47022212]\n",
      " [0.37322338 0.44631097 1.         0.49479337 0.6547976  0.52447321]\n",
      " [0.11788947 0.33833166 0.49479337 1.         0.59648798 0.45439273]\n",
      " [0.25886513 0.44378826 0.6547976  0.59648798 1.         0.62814475]\n",
      " [0.30239811 0.47022212 0.52447321 0.45439273 0.62814475 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(file1_embeddings)  # embeddings: list of vectors\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Interpretation:\n",
    "# 1.0: Vectors are identical in direction (maximum similarity).\n",
    "# 0.0: Vectors are orthogonal (no similarity).\n",
    "# -1.0: Vectors are opposite in direction (rare in NLP embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9609ecd",
   "metadata": {},
   "source": [
    "# experiment HDBSCAN:\n",
    "the results were -1, which means that everything was considered noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to retrieve the chunks from a specific file\n",
    "\n",
    "def get_file_chunks(fname):\n",
    "    results = vectorstore.get(where={\"source\": fname})\n",
    "    file_chunks = results['documents']\n",
    "    print(f\"Found {len(file_chunks)} chunks from {fname}\")\n",
    "    for chunk in file_chunks:\n",
    "        display(Markdown(f\"```\\n{chunk}\\n```\"))             # <-- display the contents of each chunk for conference\n",
    "    return file_chunks                                      # <-- return the list for further use\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e4549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 chunks from note-1-dt-r.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "entropy = -p * log2 (p) – yeah info gain is difference in entropy before and after split — okay so DT picks feature that max info gain at each node . Gini impurity also similar but faster? less bias? not 100% sure . trees go deep and then prone back? no, prune . to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". to prevent overfitting . training error low but generalization bad . CART uses binary splits – only yes/no right? sklearn.tree.DecisionTreeClassifier (max_depth=3) — yeah that ’ s what prof used . note: good to visualize trees but not for high dim data . lots of axis-aligned splits, hard to interpret when too many features . oh and trees are unstable — small data change = big model change . they said bagging can help that\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "oh and trees are unstable — small data change = big model change . they said bagging can help that — RandomForest . wait then they jumped to regularization — lasso vs ridge . ridge adds λ * sum (w²), shrinks weights, but all stay ≠ 0 . Lasso adds λ * sum (|w|) — forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "— forces some to zero . ohhh good for feature selection . balance bias-variance tradeoff . λ too big = underfit . low λ = overfit . prof wrote this on board: from sklearn.linear_model import Lasso model = Lasso (alpha=0.1) model.fit (X_train, y_train) ElasticNet = mix of both? ratio param controls mix . good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "good when multicollinearity or many small coeffs . visualize loss function — lasso diamond corners cause zeros . interesting . btw they said don ’ t scale trees but do scale for lasso etc . bcz regularization depends on magnitude . std scaling or minmax okay . might try gridsearch to tune alpha — sklearn.model_selection.GridSearchCV decision boundary of tree is step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "step-like, not smooth like linear models . ok Q: why trees overfit more than lasso? more flexible model class I think?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for File 1: [-1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# 4.1. Get embeddings chunks from File 1\n",
    "\n",
    "file1_chunks = get_file_chunks(\"note-1-dt-r.txt\")\n",
    "file1_embeddings = embeddings.embed_documents(file1_chunks)\n",
    "\n",
    "\n",
    "# 4.2. Initialize and fit HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2)\n",
    "labels = cluster.fit_predict(file1_embeddings)\n",
    "\n",
    "print(\"Cluster labels for File 1:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 chunks from note-2-ul.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "unsup = no labels . kmeans = simplest one but still used a lot . init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat . converge when no pt changes . but result depends on init + scale . scale important! feature w bigger range dominates dist calc — always standardize first . elbow method = plot inertia vs k — look for bend, but not always obvious . inertia =\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". elbow method = plot inertia vs k — look for bend, but not always obvious . inertia = sum of dist² to centroid . alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered . clustering ≠ classification . labels are not known . use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps +\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ", anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples . tricky to tune tho . forms clusters based on density, noisy pts marked as outliers (label -1). sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN (eps=0.5, min_samples=5) model.fit (X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "(X) hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative). start w all pts as indiv cluster, merge closest pairs step by step . dendrogram = tree of merges . can \"cut\" tree at diff levels = diff num clusters . linkage: single, complete, avg . sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "sklearn has AgglomerativeClustering . before clustering, can reduce dim (PCA) for speed + viz . tSNE/UMAP for 2D plot = better for human eye but not for modeling . PCA = linear, tSNE = non-linear . tSNE distorts structure globally . good for pattern discovery . spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "→ k-means in lower-dim eigenspace . nice when structure is graphy, not spherical . most clustering algos rely on distance metric — Euclidean default . alt: cosine sim (for text), manhattan, etc . eval: hard bcz no true label . silhouette best for most . DB index too . compare within/between cluster distance . can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". can also visualize clusters to judge quality . problem: k-means assumes spherical clusters, equal size . not true for real-world . if data has diff density or shapes → fails . open Q: how to know if clusters mean anything in real world? scaling is essential — StandardScaler or MinMaxScaler from sklearn . pipeline ex: from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit (\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ": from sklearn.pipeline import make_pipeline pipe = make_pipeline (StandardScaler (), KMeans (n_clusters=3)) pipe.fit (X)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for File 2: [-1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "file2_chunks = get_file_chunks(\"note-2-ul.txt\")\n",
    "file2_embeddings = embeddings.embed_documents(file1_chunks)\n",
    "\n",
    "\n",
    "# 4.2. Initialize and fit HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2)\n",
    "labels = cluster.fit_predict(file2_embeddings)\n",
    "\n",
    "print(\"Cluster labels for File 2:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 chunks from note-3-dt-r-ul.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "lecture today was fast af . started with trees . entropy vs gini impurity — diff metrics to decide best split . both OK. CART = binary tree = each node has 2 splits . sklearn uses this . code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier (criterion='entropy', max_depth=4) trees prone to overfit — esp if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "if depth unbounded . pruning = way to fix . early stopping or post-prune . bagging helps = ensemble . RandomForest = multiple trees on bootstrapped samples + rand subset of features per split . reduces variance . trees = interpretability good, but unstable to data changes . then prof jumped into regression . regularization = shrink model capacity . Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ *\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       ". Ridge = L2 norm = λ * Σ (w²). Lasso = L1 norm = λ * Σ|w| . Ridge keeps all weights ≠ 0, Lasso can zero out → sparse . Lasso good for feature selection . ElasticNet = mix of both — good if features correlated code ex: from sklearn.linear_model import ElasticNet model = ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "= ElasticNet (alpha=0.1, l1_ratio=0.5) model.fit (X, y) important: scale features before fitting regularized models — otherwise magnitudes skew the penalty . StandardScaler or RobustScaler if outliers . tune α via cross-val — use GridSearchCV or RandomizedSearchCV . metrics: RMSE, R² . underfit vs overfit — regularization helps balance bias/var tradeoff . last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "last part was unsupervised learning . clustering w/o labels . k-means = most used . init centers, assign pts, recalc, repeat . problem: sensitive to init . use k-means++ . elbow method not always clear . silhouette score better maybe . PCA + k-means often combined for vis . t-SNE only for viz — not for modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "modeling . clusters in t-SNE are sometimes fake . DBSCAN = cluster via density . can detect noise . great for shape-agnostic clusters . params hard to tune tho . hierarchical = dendrograms . use ‘ ward ’ linkage . but slow w/ big data . general note: sklearn models consistent API — fit / predict / score . also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "also: why trees don ’ t need scaling? bcz splits based on order not value . contrast w/ reg models . each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for File 1: [-1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "file3_chunks = get_file_chunks(\"note-3-dt-r-ul.txt\")\n",
    "file3_embeddings = embeddings.embed_documents(file3_chunks)\n",
    "\n",
    "\n",
    "# 4.2. Initialize and fit HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2)\n",
    "labels = cluster.fit_predict(file3_embeddings)\n",
    "\n",
    "print(\"Cluster labels for File 1:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed0279",
   "metadata": {},
   "source": [
    "### old class clusterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00110134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkClusterizer:\n",
    "    def __init__(self, persist_dir, embeddings):\n",
    "        self.persist_dir = persist_dir\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = []\n",
    "        self.labels = []\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=self.persist_dir,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "    def get_file_chunks(self, fname):\n",
    "        results = self.vectorstore.get(where={\"source\": fname})\n",
    "        self.chunks = results['documents']\n",
    "        print(f\"Found {len(self.chunks)} chunks in {fname}\")\n",
    "        for chunk in self.chunks:\n",
    "            display(Markdown(f\"```\\n{chunk}\\n```\"))\n",
    "        return self.chunks\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=2):\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        X = self.embeddings.embed_documents(self.chunks)\n",
    "        clusterizer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        self.labels = clusterizer.fit_predict(X)\n",
    "\n",
    "        for cluster_id in sorted(set(self.labels)):\n",
    "            print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "            for i, label in enumerate(self.labels):\n",
    "                if label == cluster_id:\n",
    "                    print(f\"Chunk {i+1}:\\n{self.chunks[i][:200]}{'...' if len(self.chunks[i]) > 200 else ''}\\n\")\n",
    "        return self.labels"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
