{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a2c20e",
   "metadata": {},
   "source": [
    "# Final Project: NeuroDump - thoughts organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3894bd",
   "metadata": {},
   "source": [
    "## I. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ea34",
   "metadata": {},
   "source": [
    "### 1.1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0500ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt --- UPDATE THIS AT THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99bb",
   "metadata": {},
   "source": [
    "### 1.2. Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cb99a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import ast\n",
    "\n",
    "nltk.download('punkt')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b531c",
   "metadata": {},
   "source": [
    "### 1.3. Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d133cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config paths\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          #files\n",
    "LOG_PATH = \"./data/processed_files.txt\"     #list of processed files\n",
    "PERSIST_DIR = \"./data/chroma_db\"            #embeddings database (store vectorized chunks)\n",
    "CLUSTER_DIR = \"./data/cluster_dir/\"         #clusters database\n",
    "SUBNOTES = \"./data/outputs/SUBNOTES\"        #subnotes formatted (chunks list)\n",
    "LLM_RECLUSTER = \"./data/outputs/LLM_recluster\"\n",
    "CLUSTER_NAMES_DICT = \"./data/outputs/LLM_recluster/cluster_names_dict.json\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "os.makedirs(CLUSTER_DIR, exist_ok=True)\n",
    "os.makedirs(SUBNOTES, exist_ok=True)\n",
    "os.makedirs(LLM_RECLUSTER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce7bdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "HF_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69f4e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Save JSON file \n",
    "def save_json(data, folder_path, filename):\n",
    "    \"\"\"Save data as JSON to specified folder and filename.\"\"\"\n",
    "    out_path = os.path.join(folder_path, filename)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n=== Saved JSON to: {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f084ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_base_filename(filename):\n",
    "    \"\"\"Extract base filename without extension and suffixes.\"\"\"\n",
    "    # Remove .txt extension if present\n",
    "    if filename.endswith('.txt'):\n",
    "        filename = filename[:-4]\n",
    "    \n",
    "    # Remove _clusters suffix if present\n",
    "    if filename.endswith('_clusters'):\n",
    "        filename = filename[:-9]\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52630bd8",
   "metadata": {},
   "source": [
    "# II. Craft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa5ac8",
   "metadata": {},
   "source": [
    "### 1. Input unstructured document\n",
    "Incremental version. It will process only new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9227724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files():\n",
    "    \"\"\"Load list of already processed files from log.\"\"\"\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_new_files():\n",
    "    \"\"\"Get list of new .txt files that haven't been processed yet.\"\"\"\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        return []\n",
    "    \n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c3aed",
   "metadata": {},
   "source": [
    "### 2. Split document into chunks of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafefc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using the embedding model's tokenizer.\"\"\"\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    \"\"\"Detect if a sentence contains code-like patterns.\"\"\"\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords) or\n",
    "        any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "\n",
    "# ### CHUNKING LOGIC\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    \"\"\"Split text into semantic chunks with token limits.\"\"\"\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())                    # Split by double newlines\n",
    "    chunks, current_chunk, current_tokens = [], [], 0                  # Initialize chunking variables\n",
    "    metadata = []                                                      # Store metadata for each chunk\n",
    "\n",
    "    for block in raw_blocks:                                           # Process each text block\n",
    "        sentences = nltk.sent_tokenize(block.strip())                  # Split block into sentences\n",
    "\n",
    "        for sentence in sentences:                                     # Process each sentence\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)                            # Count tokens in sentence\n",
    "\n",
    "            # Handle code-like sentences separately\n",
    "            if is_code_like(sentence):                                 # Check if sentence contains code\n",
    "                if current_chunk:                                      # Save current chunk if exists\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)                                # Add code as separate chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            # Handle conversation transitions\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:                                      # Break chunk at transition words\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            # Check if adding sentence would exceed max tokens\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))                # Save current chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0                 # Reset for new chunk\n",
    "\n",
    "            current_chunk.append(sentence)                            # Add sentence to current chunk\n",
    "            current_tokens += tokens                                  # Update token count\n",
    "\n",
    "        # Save any remaining chunk after block processing\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    # if inspect:                                                       # Optional: print chunks for debugging\n",
    "    #     for i, chunk in enumerate(chunks):\n",
    "    #         print(f\"\\n=== Chunks ===\")\n",
    "    #         print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d382bde",
   "metadata": {},
   "source": [
    "### 3. Embeddings and save vectors in Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VECTOR STORE MANAGEMENT\n",
    "class VectorStoreManager:\n",
    "    def __init__(self):\n",
    "        self.persist_dir = PERSIST_DIR                              # Store database directory path\n",
    "        self.embeddings = HF_embeddings                          # Store embedding model\n",
    "        self.vectorstore = self._load_existing()                    # Load existing DB or set to None\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Load existing Chroma database if it exists.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"\\n=== Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n=== Error loading existing Chroma DB: {e}\")\n",
    "        \n",
    "        print(\"\\n=== No existing Chroma DB found. Will create new one.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Add text chunks to vector store with embeddings.\"\"\"\n",
    "        if not self.vectorstore:                                     # Create new database if none exists\n",
    "            print(f\"\\n=== Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"\n",
    "            )\n",
    "        else:                                                        # Add to existing database\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "\n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieve chunks, embeddings, and metadata for a specific file.\"\"\"\n",
    "        results = self.vectorstore.get(\n",
    "            where={\"source\": fname},\n",
    "            include=[\"documents\", \"embeddings\", \"metadatas\"]\n",
    "        )\n",
    "        chunks = results.get('documents', [])               # Extract text chunks\n",
    "        embeddings_list = results.get('embeddings', [])     # Exttact embeddings\n",
    "        metadatas = results.get('metadatas', [])            # Extract metadata\n",
    "    \n",
    "        return chunks, embeddings_list, metadatas        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecb217",
   "metadata": {},
   "source": [
    "### 4. Clusterize embeddings with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b53ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### CLUSTERING LOGIC\n",
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings, metadata, fname):\n",
    "        self.chunks = chunks                                         # Store text chunks\n",
    "        self.embeddings = np.array(chunk_embeddings)                 # Convert embeddings to numpy array for ML operations\n",
    "        self.metadata = metadata                                     # Save metadata       \n",
    "        self.fname = fname\n",
    "        self.labels = []                                             # Initialize cluster labels\n",
    "        self.clusters = defaultdict(list)                            # cluster_id -> list of dicts {chunk, metadata}\n",
    "\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        \"\"\"Cluster chunks using HDBSCAN algorithm.\"\"\"\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if len(self.chunks) < min_cluster_size:                      # Not enough chunks to cluster\n",
    "            print(f\"\\n=== Not enough chunks to cluster ({len(self.chunks)} < {min_cluster_size}) ===\")\n",
    "            self.labels = [-1] * len(self.chunks)                   # Assign all to noise\n",
    "            return self.labels\n",
    "\n",
    "        # Ensure embeddings are 2D\n",
    "        if self.embeddings.ndim == 1:                               # Reshape 1D embeddings to 2D\n",
    "            self.embeddings = self.embeddings.reshape(1, -1)\n",
    "            \n",
    "        #print(f\"\\n===> Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "\n",
    "        # Perform clustering\n",
    "        clusterizer = hdbscan.HDBSCAN(                                     # Initialize HDBSCAN clustering algorithm\n",
    "            min_cluster_size=min_cluster_size,                             # Minimum number of points in a cluster\n",
    "            min_samples=min_samples,                                       # Minimum samples in neighborhood for core point\n",
    "            metric='euclidean'                                             # Distance metric for clustering\n",
    "        )\n",
    "        self.labels = clusterizer.fit_predict(self.embeddings)             # Fit model and predict cluster labels = generate cluster labels\n",
    "\n",
    "        # Display results\n",
    "        num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)  # Count clusters (exclude noise)\n",
    "        print(f\"\\n=== Found {num_clusters} clusters\")\n",
    "        print(\"\\n=== Cluster distribution ===\")\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            cname = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "            print(f\"{cname}: {count} chunks\")    \n",
    "                    \n",
    "        # Show cluster contents\n",
    "        for cluster_id in sorted(set(self.labels)):\n",
    "            cluster_name = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "            print(f\"\\n=== {cluster_name} ===\")\n",
    "\n",
    "            for i, label in enumerate(self.labels):\n",
    "                if label == cluster_id:\n",
    "                    # Save the chunk into the cluster dictionary with global ID\n",
    "                    self.clusters[str(cluster_id)].append({\n",
    "                        \"chunk_id\": i,                      # Global ID\n",
    "                        \"chunk\": self.chunks[i],\n",
    "                        \"metadata\": self.metadata[i]\n",
    "                    })\n",
    "\n",
    "                    # Display clearly: Chunk [global id]\n",
    "                    display(Markdown(f\"**Chunk {i}:** {self.chunks[i]}\"))\n",
    "            \n",
    "        return self.labels                                   # Return cluster labels\n",
    "        \n",
    "    def save_clusters(self):\n",
    "        filename = self.fname.replace(\".txt\", \"_clusters.json\")\n",
    "        return save_json(self.clusters, CLUSTER_DIR, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216f8cf",
   "metadata": {},
   "source": [
    "### 5. Read notes clustered by HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Read notes clustered by HDBSCAN (Updated to work with any cluster file)\n",
    "def read_note_clusters(clusters_file_path):\n",
    "    \"\"\"Read HDBSCAN clusters file from any path\"\"\"\n",
    "    with open(clusters_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        note_clusters = f.read()\n",
    "    return note_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d816696",
   "metadata": {},
   "source": [
    "### 6. Define system prompts and initiate LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c1bc8",
   "metadata": {},
   "source": [
    "#### 6.1. Prompt: recluster and generate cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02965bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_RECLUSTER = \"\"\"\n",
    "You are a smart assistant helping organize fragmented personal notes.\n",
    "\n",
    "The notes were previously split into chunks and clustered semantically. Each original note has its own clusters, saved as JSON files inside: ./data/cluster_dir/. Your task is to refine those clusters and give them a name.\n",
    "\n",
    "1. REORGANIZE CLUSTERS PER FILE:\n",
    "- You may SPLIT or MERGE clusters WITHIN EACH FILE to better reflect topic boundaries.\n",
    "- Keep a balance when creating the new clusters. The topic should NOT be too granular (e.g., splitting KMeans into 3 separate clusters), but also not too broad (e.g., grouping all machine learning together).\n",
    "- The goal is to create meaningful, reusable sub-notes by topic. Each cluster should contain all content from the original note that belongs to that topic.\n",
    "- For example, all KMeans content from a note should be together, not scattered across separate clusters.\n",
    "- Imagine someone searching \"KMeans\": everything relevant from the note should be in a single rewritten cluster.\n",
    "\n",
    "2. CLUSTER REORGANIZATION OUTPUT:\n",
    "2.1. REASONING:\n",
    "- provide a short reasoning section explaining what you split, merged, or reassigned and why. Format it as normal text.\n",
    "2.2. \n",
    "- Then output A CLEAR JSON OBJECT, with label exactly like this:\n",
    "\n",
    "NOTE_RECLUSTERED:\n",
    "{ ... }\n",
    "\n",
    "2.3.\n",
    "- Then output a SECOND JSON OBJECT, with label exactly like this:\n",
    "\n",
    "CLUSTER_NAMES:\n",
    "{ ... }\n",
    "\n",
    "\n",
    "- NOTE_RECLUSTERED contains the new clustering structure, following the same format as the input cluster JSON.\n",
    "\n",
    "- CLUSTER_NAMES is a dictionary mapping new cluster IDs created during reclustering (e.g., \"cluster_0\") to topic names.\n",
    "\n",
    "3. FORMATTING RULES (STRICT):\n",
    "- Keep the reasoning at the top.\n",
    "- After reasoning, print CLUSTER_NAMES: followed by the RAW JSON object (NO code blocks, NO quotes, NO markdown).\n",
    "- Then print NOTE_RECLUSTERED: followed by the RAW JSON (same rules).\n",
    "- DO NOT use triple backticks, ```json, or any Markdown formatting. Just the plain, raw JSON object after the tag.\n",
    "- Make sure CLUSTER_NAMES and NOTE_RECLUSTERED appear exactly like shown. They are anchors for parsing.\n",
    "- CLUSTER_NAMES topic names must be FOLDER-SAFE (use underscores, no spaces or special chars).\n",
    "DO NOT use placeholder text like '...' inside JSON.\n",
    "DO NOT include comments like `// ...` inside JSON.\n",
    "Make sure all JSON is valid and can be parsed with json.loads().\n",
    "- Example:\n",
    "\n",
    "CLUSTER_NAMES:\n",
    "{ \"cluster_0\": \"unsupervised_learning\", ... }\n",
    "\n",
    "NOTE_RECLUSTERED:\n",
    "{ \"cluster_0\": [ ... ], ... }\n",
    "\n",
    "You will receive a single note cluster JSON. Analyze and return the result using the format above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b287f",
   "metadata": {},
   "source": [
    "#### 6.2. Initiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6ccfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Ollama\n",
    "def query_ollama(model: str, prompt: str) -> str:\n",
    "    \"\"\"Initiate Ollama\"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=prompt)\n",
    "    if stderr:\n",
    "        print(\"OLLAMA STDERR:\", stderr)\n",
    "    return stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c614c",
   "metadata": {},
   "source": [
    "### 7. Extract response and save CLUSTER_NAMES and NOTE_x_RECLUSTERED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15cfe8",
   "metadata": {},
   "source": [
    "# FUNCTION WHEN I TRY TO SAVE WITH A SPECIFIC NAME + UPDATE THE EXISTIING CLUSTER_NAMES_DICT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc505d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response_and_save(response_text, fname):\n",
    "    \"\"\"Parse LLM response and save RECLUSTERED output + update CLUSTER_NAMES_DICT.json\"\"\"\n",
    "    \n",
    "    base_fname = extract_base_filename(os.path.basename(fname))\n",
    "\n",
    "    # ==== Extract CLUSTER_NAMES ====\n",
    "    cluster_names_match = re.search(r'CLUSTER_NAMES:\\s*(\\{.*?\\})', response_text, re.DOTALL)\n",
    "    if cluster_names_match:\n",
    "        cluster_names_str = cluster_names_match.group(1)\n",
    "        try:\n",
    "            cluster_names = json.loads(cluster_names_str)\n",
    "        except json.JSONDecodeError:\n",
    "            cluster_names = parse_complex_json(cluster_names_str)\n",
    "    else:\n",
    "        cluster_names = {}\n",
    "\n",
    "    # ==== Extract NOTE_RECLUSTERED ====\n",
    "    note_reclustered_match = re.search(r'NOTE_RECLUSTERED:\\s*(\\{.*?\\})', response_text, re.DOTALL)\n",
    "    if note_reclustered_match:\n",
    "        note_reclustered_str = note_reclustered_match.group(1)\n",
    "        try:\n",
    "            note_reclustered = json.loads(note_reclustered_str)\n",
    "        except json.JSONDecodeError:\n",
    "            note_reclustered = parse_complex_json(note_reclustered_str)\n",
    "    else:\n",
    "        note_reclustered = {}\n",
    "\n",
    "    # ==== Save NOTE_RECLUSTERED ====\n",
    "    reclustered_path = os.path.join(LLM_RECLUSTER, f\"{base_fname}_RECLUSTERED.json\")\n",
    "    with open(reclustered_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(note_reclustered, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # ==== Update or create CLUSTER_NAMES_DICT.json ====\n",
    "    if os.path.exists(CLUSTER_NAMES_DICT):\n",
    "        with open(CLUSTER_NAMES_DICT, 'r', encoding='utf-8') as f:\n",
    "            clusters_NAMES_dict = json.load(f)\n",
    "    else:\n",
    "        clusters_NAMES_dict = {}\n",
    "\n",
    "    # Add new entries (overwrite if repeated)\n",
    "    clusters_NAMES_dict.update(cluster_names)\n",
    "\n",
    "    with open(CLUSTER_NAMES_DICT, 'w', encoding='utf-8') as f:\n",
    "        json.dump(clusters_NAMES_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"Files saved:\")\n",
    "    print(f\"  - {reclustered_path}\")\n",
    "    print(f\"  - {CLUSTER_NAMES_DICT}\")\n",
    "\n",
    "    return reclustered_path, note_reclustered, CLUSTER_NAMES_DICT\n",
    "\n",
    "\n",
    "\n",
    "def parse_complex_json(json_str):\n",
    "    \"\"\"Safer fallback parser.\"\"\"\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return ast.literal_eval(json_str)\n",
    "        except:\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd07b7",
   "metadata": {},
   "source": [
    "# test my function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf9116e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_test = \"\"\"\n",
    "REASONING:\n",
    "The original clusters seem to be dividing the content based on specific techniques or models rather than topics, making it difficult for a search on a topic like \"Decision Trees\" or \"Unsupervised Learning\". To make these clusters more meaningful and reusable, I have merged the initial three clusters (0, 1, 2) which focus on Decision Trees, their properties, and related topics. Similarly, the next two clusters (3, 4) are grouped together as they discuss tree-related concepts like pruning and reducing variance. The final cluster (5) seems to stand alone, discussing regularization. The two remaining clusters (6, \"unassigned\") focus on Ridge and Lasso regression models and are merged into one, as both deal with the same topic of Regularization.\n",
    "CLUSTER_NAMES:\n",
    "{\n",
    "  \"cluster_0\": \"decision_trees\",\n",
    "  \"cluster_1\": \"tree_related\",\n",
    "  \"cluster_2\": \"regularization\"\n",
    "}\n",
    "NOTE_RECLUSTERED:\n",
    "{\n",
    "  \"cluster_0\": [\n",
    "    {\n",
    "      \"chunk_id\": 0,\n",
    "      \"chunk\": \"lecture today was fast af. started with trees. entropy vs gini impurity — diff metrics to decide best split.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 1,\n",
    "      \"chunk\": \"both OK. CART = binary tree = each node has 2 splits.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 2,\n",
    "      \"chunk\": \"sklearn uses this.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 3,\n",
    "      \"chunk\": \"trees = interpretability good, but unstable to data changes.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 4,\n",
    "      \"chunk\": \"pruning = way to fix.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 5,\n",
    "      \"chunk\": \"early stopping or post-prune.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 6,\n",
    "      \"chunk\": \"reduces variance.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 7,\n",
    "      \"chunk\": \"RandomForest = multiple trees on bootstrapped samples + rand subset of features per split.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 8,\n",
    "      \"chunk\": \"hierarchical = dendrograms.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 9,\n",
    "      \"chunk\": \"trees don’t need scaling? bcz splits based on order not value. contrast w/ reg models.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 10,\n",
    "      \"chunk\": \"then prof jumped into regression.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"cluster_1\": [\n",
    "    {\n",
    "      \"chunk_id\": 15,\n",
    "      \"chunk\": \"ElasticNet = mix of both — good if features correlated\\ncode ex:\\nfrom sklearn.linear_model import ElasticNet model = ElasticNet(alpha=0.1, l1_ratio=0.5) model.fit(X, y)\\nimportant: scale features before fitting regularized models — otherwise magnitudes skew the penalty.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 17,\n",
    "      \"chunk\": \"k-means = most used.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 18,\n",
    "      \"chunk\": \"init centers, assign pts, recalc, repeat. problem: sensitive to init. use k-means++. elbow method not always clear. silhouette score better maybe. PCA + k-means often combined for vis. t-SNE only for viz — not for modeling. clusters in t-SNE are sometimes fake.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"cluster_2\": [\n",
    "    {\n",
    "      \"chunk_id\": 19,\n",
    "      \"chunk\": \"DBSCAN = cluster via density.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 20,\n",
    "      \"chunk\": \"can detect noise. great for shape-agnostic clusters. params hard to tune tho.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 21,\n",
    "      \"chunk\": \"bagging helps = ensemble.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 16,\n",
    "      \"chunk\": \"StandardScaler or RobustScaler if outliers. tune α via cross-val — use GridSearchCV. early stopping or post-prune.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 12,\n",
    "      \"chunk\": \"Ridge = L2 norm = Σ(w²).\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 13,\n",
    "      \"chunk\": \"Lasso = L1 norm = Σ|w|.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 14,\n",
    "      \"chunk\": \"Ridge keeps all weights ≠ 0, Lasso can zero out → sparse. Lasso good for feature selection.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb774f9",
   "metadata": {},
   "source": [
    "# my function save the cluster_names_dict , but the reclustered.json is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved:\n",
      "  - ./data/outputs/LLM_recluster/note_3_dt_r_ul_clusters.json_RECLUSTERED.json\n",
      "  - ./data/outputs/LLM_recluster/cluster_names_dict.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reclustered_path, note_reclustered, CLUSTER_NAMES_DICT = parse_response_and_save(response_text=response_test, fname = \"note_3_dt_r_ul_clusters.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c3e08",
   "metadata": {},
   "source": [
    "# FUNCTION THAT WORKS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d524abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved:\n",
      "  - ./cluster_names.json\n",
      "  - ./note_reclustered.json\n",
      "CLUSTER_NAMES:\n",
      "{\n",
      "  \"cluster_0\": \"decision_trees\",\n",
      "  \"cluster_1\": \"tree_related\",\n",
      "  \"cluster_2\": \"regularization\"\n",
      "}\n",
      "\n",
      "NOTE_RECLUSTERED:\n",
      "{\n",
      "  \"cluster_0\": [\n",
      "    {\n",
      "      \"chunk_id\": 0,\n",
      "      \"chunk\": \"lecture today was fast af. started with trees. entropy vs gini impurity \\u2014 diff metrics to decide best split.\",\n",
      "      \"metadata\": {\n",
      "        \"source\": \"note_3_dt_r_ul.txt\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 1,\n",
      "      \"chunk\": \"both OK. CART = binary tree = each node has 2 splits.\",\n",
      "      \"metadata\": {\n",
      "        \"source\": \"note_3_dt_r_ul.txt\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"cluster_1\": [\n",
      "    {\n",
      "      \"chunk_id\": 15,\n",
      "      \"chunk\": \"ElasticNet = mix of both \\u2014 good if features correlated\",\n",
      "      \"metadata\": {\n",
      "        \"source\": \"note_3_dt_r_ul.txt\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"cluster_2\": [\n",
      "    {\n",
      "      \"chunk_id\": 19,\n",
      "      \"chunk\": \"DBSCAN = cluster via density.\",\n",
      "      \"metadata\": {\n",
      "        \"source\": \"note_3_dt_r_ul.txt\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_response(response_text):\n",
    "\n",
    "    \n",
    "    # Extract CLUSTER_NAMES section\n",
    "    cluster_names_match = re.search(r'CLUSTER_NAMES:\\s*(\\{[^}]*\\})', response_text, re.DOTALL)\n",
    "    if cluster_names_match:\n",
    "        cluster_names_str = cluster_names_match.group(1)\n",
    "        try:\n",
    "            cluster_names = json.loads(cluster_names_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle potential formatting issues\n",
    "            cluster_names = eval(cluster_names_str)\n",
    "    else:\n",
    "        cluster_names = {}\n",
    "    \n",
    "    # Extract NOTE_RECLUSTERED section\n",
    "    note_reclustered_match = re.search(r'NOTE_RECLUSTERED:\\s*(\\{.*\\})', response_text, re.DOTALL)\n",
    "    if note_reclustered_match:\n",
    "        note_reclustered_str = note_reclustered_match.group(1)\n",
    "        try:\n",
    "            note_reclustered = json.loads(note_reclustered_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle potential formatting issues - more complex parsing needed\n",
    "            note_reclustered = parse_complex_json(note_reclustered_str)\n",
    "    else:\n",
    "        note_reclustered = {}\n",
    "    \n",
    "    return cluster_names, note_reclustered\n",
    "\n",
    "def save_to_files(cluster_names, note_reclustered, output_dir=\"./\"):\n",
    "    \"\"\"\n",
    "    Save CLUSTER_NAMES and NOTE_RECLUSTERED to separate JSON files.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save CLUSTER_NAMES\n",
    "    cluster_names_file = os.path.join(output_dir, \"cluster_names.json\")\n",
    "    with open(cluster_names_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cluster_names, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save NOTE_RECLUSTERED\n",
    "    note_reclustered_file = os.path.join(output_dir, \"note_reclustered.json\")\n",
    "    with open(note_reclustered_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(note_reclustered, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Files saved:\")\n",
    "    print(f\"  - {cluster_names_file}\")\n",
    "    print(f\"  - {note_reclustered_file}\")\n",
    "\n",
    "def parse_complex_json(json_str):\n",
    "    \"\"\"\n",
    "    Helper function to parse complex JSON with potential formatting issues.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try direct JSON parsing first\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # If that fails, try eval (be careful with this in production)\n",
    "        try:\n",
    "            return eval(json_str)\n",
    "        except:\n",
    "            # If all else fails, return empty dict\n",
    "            return {}\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Your response text\n",
    "    response = \"\"\"\n",
    "REASONING:\n",
    "The original clusters seem to be dividing the content based on specific techniques or models rather than topics, making it difficult for a search on a topic like \"Decision Trees\" or \"Unsupervised Learning\". To make these clusters more meaningful and reusable, I have merged the initial three clusters (0, 1, 2) which focus on Decision Trees, their properties, and related topics. Similarly, the next two clusters (3, 4) are grouped together as they discuss tree-related concepts like pruning and reducing variance. The final cluster (5) seems to stand alone, discussing regularization. The two remaining clusters (6, \"unassigned\") focus on Ridge and Lasso regression models and are merged into one, as both deal with the same topic of Regularization.\n",
    "\n",
    "CLUSTER_NAMES:\n",
    "{\n",
    "  \"cluster_0\": \"decision_trees\",\n",
    "  \"cluster_1\": \"tree_related\",\n",
    "  \"cluster_2\": \"regularization\"\n",
    "}\n",
    "\n",
    "NOTE_RECLUSTERED:\n",
    "{\n",
    "  \"cluster_0\": [\n",
    "    {\n",
    "      \"chunk_id\": 0,\n",
    "      \"chunk\": \"lecture today was fast af. started with trees. entropy vs gini impurity — diff metrics to decide best split.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"chunk_id\": 1,\n",
    "      \"chunk\": \"both OK. CART = binary tree = each node has 2 splits.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"cluster_1\": [\n",
    "    {\n",
    "      \"chunk_id\": 15,\n",
    "      \"chunk\": \"ElasticNet = mix of both — good if features correlated\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"cluster_2\": [\n",
    "    {\n",
    "      \"chunk_id\": 19,\n",
    "      \"chunk\": \"DBSCAN = cluster via density.\",\n",
    "      \"metadata\": {\n",
    "        \"source\": \"note_3_dt_r_ul.txt\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "    # Parse the response\n",
    "    cluster_names, note_reclustered = parse_response(response)\n",
    "    \n",
    "    # Save to separate files\n",
    "    save_to_files(cluster_names, note_reclustered)\n",
    "    \n",
    "    print(\"CLUSTER_NAMES:\")\n",
    "    print(json.dumps(cluster_names, indent=2))\n",
    "    print(\"\\nNOTE_RECLUSTERED:\")\n",
    "    print(json.dumps(note_reclustered, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7b833",
   "metadata": {},
   "source": [
    "### 8. Generate sub-notes from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c5e9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_clusters_to_formatted_subnotes(reclustered_file_path=LLM_RECLUSTER, cluster_names_dict=CLUSTER_NAMES_DICT, output_folder=SUBNOTES):\n",
    "#     \"\"\"Split reclustered JSON file by cluster and save as formatted text files with original filename\"\"\"\n",
    "    \n",
    "#     # Get original filename base\n",
    "#     original_fname = get_original_filename_from_clusters(reclustered_file_path)\n",
    "    \n",
    "#     # Load the JSON file\n",
    "#     with open(reclustered_file_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     formatted_subnotes = []\n",
    "    \n",
    "#     # Process each cluster\n",
    "#     for cluster_id, chunks in data.items():\n",
    "#         # Get cluster name from cluster_names_dict\n",
    "#         cluster_name = cluster_names_dict.get(cluster_id, cluster_id)\n",
    "        \n",
    "#         # Create the output file path with original filename preserved\n",
    "#         output_filename = f\"{original_fname}_{cluster_name}.txt\"\n",
    "#         output_file = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "#         # Format the content with simple bullet points\n",
    "#         with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#             f.write(f\"Topic: {cluster_name}\\n\")\n",
    "#             f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "#             # Write each chunk as a simple bullet point\n",
    "#             for chunk_data in chunks:\n",
    "#                 chunk_text = chunk_data.get('chunk', '').strip()\n",
    "#                 if chunk_text:  # Only write non-empty chunks\n",
    "#                     f.write(f\"• {chunk_text}\\n\")\n",
    "            \n",
    "#             f.write(f\"\\n[Total: {len(chunks)} notes]\\n\")\n",
    "\n",
    "#         formatted_subnotes.append(output_file)\n",
    "#         print(f\"Created {output_file} with {len(chunks)} chunks\")\n",
    "    \n",
    "#     print(f\"\\nAll formatted cluster files saved to '{output_folder}' folder\")\n",
    "#     return formatted_subnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de8c8b",
   "metadata": {},
   "source": [
    "# Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ebb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### MAIN PROCESSING PIPELINE\n",
    "def process_file(fname):\n",
    "    \"\"\"Process a single file through the complete pipeline.\"\"\"\n",
    "    print(f\"\\n\\n=============== PROCESSING FILE: {fname} ===============\")\n",
    "\n",
    "    # Step 1: Read file\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Step 2: Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)        # Split text into chunks\n",
    "    print(f\"\\n=== Created {len(chunks)} chunks\")\n",
    "\n",
    "    # Step 3: Store in vector database\n",
    "    vector_manager = VectorStoreManager()    # Initialize vector store\n",
    "    vector_manager.add_chunks(chunks, metadata)                     # Add chunks with embeddings\n",
    "    print(f\"\\n=== Stored {len(chunks)} chunks in Chroma DB\")\n",
    "    \n",
    "    # Step 3.1: Mark as processed\n",
    "    with open(LOG_PATH, \"a\") as log:                                # Log processed file\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    # Step 4: Cluster chunks\n",
    "    print(f\"\\n=== CLUSTERING CHUNKS FROM {fname}\")\n",
    "    chunks, embeddings, metadatas = vector_manager.get_chunks_with_embeddings(fname) # Retrieve stored data\n",
    "\n",
    "    clusterizer = ChunkClusterizer(chunks, embeddings, metadatas, fname)             # Initialize clustering\n",
    "    clusterizer.cluster_chunks()                                            # Perform clustering\n",
    "    clusterizer.save_clusters()                                             # Save clusters\n",
    "\n",
    "    # STEP 5: Read clustered JSON file\n",
    "    clusters_path = os.path.join(CLUSTER_DIR, fname.replace(\".txt\", \"_clusters.json\"))\n",
    "    note_clusters = read_note_clusters(clusters_path)\n",
    "\n",
    "    # STEP 6: Query LLM with SYSTEM_PROMPT_RECLUSTER\n",
    "    model = \"mistral\"\n",
    "    prompt = SYSTEM_PROMPT_RECLUSTER + f\"\\n\\nHERE IS THE CLUSTER DATA FROM {fname}:\\n\" + note_clusters + \"\\n\\nPLEASE PERFORM THE RECLUSTERING AND CLUSTER NAMING TASKS.\"\n",
    "    response = query_ollama(model, prompt)\n",
    "\n",
    "    # STEP 6.1: Display reasoning(Markdown)\n",
    "    reasoning_part = response.split(\"CLUSTER_NAMES:\")[0]\n",
    "    display(Markdown(reasoning_part.strip()))\n",
    "\n",
    "    # STEP 7: Parse and save JSON outputs\n",
    "    cluster_names_dict_file, note_reclustered, note_reclustered_file = parse_response_and_save(response_text=response, fname=fname)\n",
    "\n",
    "\n",
    "\n",
    "def process_all_new():\n",
    "    \"\"\"Process all new files in the input directory.\"\"\"\n",
    "    new_files = get_new_files()                                     # Get list of unprocessed files\n",
    "    \n",
    "    if not new_files:                                               # Exit early if no new files\n",
    "        print(\"\\n=== No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Found {len(new_files)} new files: {new_files}\")\n",
    "    \n",
    "    for fname in new_files:                                         # Process each new file\n",
    "        process_file(fname)\n",
    "    \n",
    "    print(f\"\\n=== Completed processing {len(new_files)} files\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FOR LATER: \"MAIN GUARD\" OR \"ENTRY POINT PATTERN\" \n",
    "# EXECUTION \n",
    "#if __name__ == \"__main__\":\n",
    "#    process_all_new()                                               # Run the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda7942",
   "metadata": {},
   "source": [
    "#### Option 1: Process all new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04884e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Found 2 new files: ['note_1_ul.txt', 'note_3_dt_r_ul.txt']\n",
      "\n",
      "\n",
      "=============== PROCESSING FILE: note_1_ul.txt ===============\n",
      "\n",
      "=== Created 20 chunks\n",
      "\n",
      "=== Loading existing Chroma DB from ./data/chroma_db\n",
      "\n",
      "=== Stored 20 chunks in Chroma DB\n",
      "\n",
      "=== CLUSTERING CHUNKS FROM note_1_ul.txt\n",
      "\n",
      "=== Found 4 clusters\n",
      "\n",
      "=== Cluster distribution ===\n",
      "Noise: 2 chunks\n",
      "Cluster 0: 3 chunks\n",
      "Cluster 1: 3 chunks\n",
      "Cluster 2: 9 chunks\n",
      "Cluster 3: 3 chunks\n",
      "\n",
      "=== Noise ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 0:** unsup = no labels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:** elbow method = plot inertia vs k — look for bend, but not always obvious."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 0 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 12:** tSNE/UMAP for 2D plot = better for human eye but not for modeling."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 13:** PCA = linear, tSNE = non-linear."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 14:** tSNE distorts structure globally. good for pattern discovery."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 1 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 9:** dendrogram = tree of merges."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 10:** can \"cut\" tree at diff levels = diff num clusters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 17:** if data has diff density or shapes → fails."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 2 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:** kmeans = simplest one but still used a lot."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 5:** alt metric = silhouette score — between -1 and 1. close to 1 = well-clustered."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 6:** clustering ≠ classification. labels are not known. use cases: market segmentation, gene expr clustering, anomaly detection (esp dbscan). DBSCAN better for weird shapes, dense clusters — uses eps + min_samples. tricky to tune tho. forms clusters based on density, noisy pts marked as outliers (label -1)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 7:** sklearn DBSCAN ex: from sklearn.cluster import DBSCAN model = DBSCAN(eps=0.5, min_samples=5) model.fit(X)\n",
       "hierarchical clustering = agglomerative or divisive — we focus on bottom-up (agglomerative)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 11:** linkage: single, complete, avg. sklearn has AgglomerativeClustering. before clustering, can reduce dim (PCA) for speed + viz."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 15:** spectral clustering = build similarity graph → Laplacian → eigenvectors → k-means in lower-dim eigenspace."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 16:** nice when structure is graphy, not spherical. most clustering algos rely on distance metric — Euclidean default. alt: cosine sim (for text), manhattan, etc. eval: hard bcz no true label. silhouette best for most. DB index too. compare within/between cluster distance. can also visualize clusters to judge quality. problem: k-means assumes spherical clusters, equal size. not true for real-world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 18:** open Q: how to know if clusters mean anything in real world? scaling is essential — StandardScaler or MinMaxScaler from sklearn."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 19:** pipeline ex:\n",
       "from sklearn.pipeline import make_pipeline pipe = make_pipeline(StandardScaler(), KMeans(n_clusters=3)) pipe.fit(X)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 3 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:** init k centroids randomly (k-means++ better), assign pts, recalc centroids, repeat. converge when no pt changes. but result depends on init + scale. scale important! feature w bigger range dominates dist calc — always standardize first."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 4:** inertia = sum of dist² to centroid."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 8:** start w all pts as indiv cluster, merge closest pairs step by step."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saved JSON to: ./data/cluster_dir/note_1_ul_clusters.json\n",
      "OLLAMA STDERR: \u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Reasoning:\n",
       "The original cluster splits the note by the order of appearance, but some topics are not separated appropriately. For example, all KMeans content is scattered across different clusters instead of being grouped together. I will merge and split clusters to create meaningful sub-notes by topic."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saved JSON to: ./data/outputs/LLM_recluster/CLUSTER_NAMES_DICT.json\n",
      "\n",
      "=== Saved JSON to: ./data/outputs/LLM_recluster/note_1_ul_RECLUSTERED.json\n",
      "Files saved:\n",
      "  - ./data/outputs/LLM_recluster/CLUSTER_NAMES_DICT.json\n",
      "  - ./data/outputs/LLM_recluster/note_1_ul_RECLUSTERED.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_all_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 60\u001b[0m, in \u001b[0;36mprocess_all_new\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m new_files:                                         \u001b[38;5;66;03m# Process each new file\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Completed processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     42\u001b[0m display(Markdown(reasoning_part\u001b[38;5;241m.\u001b[39mstrip()))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# STEP 7: Parse and save JSON outputs\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m cluster_names_dict, recluster_data \u001b[38;5;241m=\u001b[39m parse_response_and_save(response, fname)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "process_all_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318c563",
   "metadata": {},
   "source": [
    "#### Option 2: Process 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\".txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
