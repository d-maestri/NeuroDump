{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a2c20e",
   "metadata": {},
   "source": [
    "# Final Project: NeuroDump - thoughts organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3894bd",
   "metadata": {},
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ea34",
   "metadata": {},
   "source": [
    "### 1.1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0500ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt --- UPDATE THIS AT THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99bb",
   "metadata": {},
   "source": [
    "### 1.2. Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb28ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e9b531c",
   "metadata": {},
   "source": [
    "### 1.3. Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229a806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca80c63",
   "metadata": {},
   "source": [
    "# ### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb99a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec73a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52630bd8",
   "metadata": {},
   "source": [
    "# 2. Craft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1fd3a",
   "metadata": {},
   "source": [
    "   1. Input unstructured document file\n",
    "      1. Config paths\n",
    "      2. Check if the input is already processed or is a new file\n",
    "      3. Only continue if it's a new file\n",
    "   2. Split document into chunks of tokens\n",
    "   3. Embed all chunks\n",
    "      1. sentence-transformers (all-MiniLM-L6-v2)\n",
    "      2. save embeddings into Chroba DB\n",
    "      3. keep metada to track original document file for each chunk\n",
    "   4. Cluster the embeddings by semantic similarity (based on all existing vectors in Chroma DB, not only from new file)\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa5ac8",
   "metadata": {},
   "source": [
    "### 1. Input unstructured document\n",
    "Incremental version. It will process only new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d133cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config paths\n",
    "\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          #files\n",
    "LOG_PATH = \"./data/processed_files.txt\"     #list of processed files\n",
    "PERSIST_DIR = \"./data/chroma_db\"            #embeddings database (store vectorized chunks)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5572a0a",
   "metadata": {},
   "source": [
    "#### 1.2. Check if the input is already processed or is a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9227724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files():\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_new_files():\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        print(f\"Folder {FOLDER_PATH} does not exist.\")\n",
    "        return []\n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c3aed",
   "metadata": {},
   "source": [
    "### 2. Split document into chunks of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafefc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & chunk helpers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords)\n",
    "        or any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate chunks\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    chunks, current_chunk, current_tokens = [], [], 0\n",
    "    metadata = []\n",
    "\n",
    "    for block in raw_blocks:\n",
    "        sentences = nltk.sent_tokenize(block.strip())\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)\n",
    "\n",
    "            if is_code_like(sentence):\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0\n",
    "\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += tokens\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    if inspect:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d382bde",
   "metadata": {},
   "source": [
    "### 3. Embed all chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c2be1",
   "metadata": {},
   "source": [
    "#### 3.1. Initialize sentence-transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b95d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15a414",
   "metadata": {},
   "source": [
    "#### 3.2. Save embeddings into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreManager:\n",
    "    def __init__(self, persist_dir, embeddings):\n",
    "        self.persist_dir = persist_dir\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstore = self._load_existing()\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Attempts to load an existing Chroma DB from the persist directory.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"  # Add this line\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing Chroma DB: {e}\")\n",
    "            print(\"Will create a new one.\")\n",
    "        \n",
    "        print(\"\\n-> No existing Chroma DB found. A new one will be created when chunks are added.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Adds chunks to the vector store, creating it if it doesn't exist.\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"No chunks to add.\")\n",
    "            return\n",
    "\n",
    "        if not self.vectorstore:\n",
    "            print(f\"\\n-> Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,  # Changed from embedding_function\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"   # Add this line\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "    def get_chunks_by_file(self, fname):\n",
    "        \"\"\"Retrieves all chunks associated with a specific source file.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vectorstore is not initialized. Cannot retrieve chunks.\")\n",
    "            return []\n",
    "        results = self.vectorstore.get(where={\"source\": fname})\n",
    "        return results.get('documents', []) # Safely get documents\n",
    "\n",
    "    def get_all_chunks(self):\n",
    "        \"\"\"Retrieves all chunks from the vector store.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vectorstore is not initialized. Cannot retrieve chunks.\")\n",
    "            return []\n",
    "        results = self.vectorstore.get(include=[\"documents\"])\n",
    "        return results.get('documents', [])\n",
    "    \n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieves chunks and their embeddings for a specific file.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vectorstore is not initialized. Cannot retrieve chunks.\")\n",
    "            return [], []\n",
    "        \n",
    "        try:\n",
    "            results = self.vectorstore.get(\n",
    "                where={\"source\": fname},\n",
    "                include=[\"documents\", \"embeddings\"]\n",
    "            )\n",
    "            chunks = results.get('documents', [])\n",
    "            embeddings_list = results.get('embeddings', [])\n",
    "            \n",
    "            if not chunks:\n",
    "                print(f\"No chunks found for file: {fname}\")\n",
    "                return [], []\n",
    "            \n",
    "            if embeddings_list is None or len(embeddings_list) == 0:\n",
    "                print(f\"No embeddings found for file: {fname}. Computing embeddings...\")\n",
    "                # Fallback: compute embeddings manually if not stored\n",
    "                embeddings_list = [self.embeddings.embed_query(chunk) for chunk in chunks]\n",
    "            \n",
    "            return chunks, embeddings_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving chunks with embeddings for file {fname}: {e}\")\n",
    "            return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecb217",
   "metadata": {},
   "source": [
    "### 4. Cluster the embeddings by semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "108e9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings):\n",
    "        self.chunks = chunks\n",
    "        self.embeddings = np.array(chunk_embeddings) if chunk_embeddings else None\n",
    "        self.labels = []\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        \n",
    "        if not self.chunks:\n",
    "            print(\"No chunks to cluster.\")\n",
    "            return []\n",
    "            \n",
    "        if len(self.chunks) < min_cluster_size:\n",
    "            print(f\"Not enough chunks to cluster. Need at least {min_cluster_size}, but have {len(self.chunks)}.\")\n",
    "            self.labels = [-1] * len(self.chunks)\n",
    "            return self.labels\n",
    "\n",
    "        # Fix: Check numpy array properly\n",
    "        if self.embeddings is None or self.embeddings.size == 0:\n",
    "            print(\"No embeddings available for clustering.\")\n",
    "            self.labels = [-1] * len(self.chunks)\n",
    "            return self.labels\n",
    "\n",
    "        try:\n",
    "            # Ensure embeddings is a 2D array\n",
    "            if self.embeddings.ndim == 1:\n",
    "                self.embeddings = self.embeddings.reshape(1, -1)\n",
    "            \n",
    "            print(f\"Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "            clusterizer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size, \n",
    "                min_samples=min_samples,\n",
    "                metric='euclidean'\n",
    "            )\n",
    "            self.labels = clusterizer.fit_predict(self.embeddings)\n",
    "\n",
    "            num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n",
    "            print(f\"\\nFound {num_clusters} clusters in the document.\")\n",
    "            print(f\"Cluster distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "            \n",
    "            for cluster_id in sorted(set(self.labels)):\n",
    "                cluster_name = \"Noise (ungrouped chunks)\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "                chunk_count = sum(1 for label in self.labels if label == cluster_id)\n",
    "                print(f\"\\n--- {cluster_name} ({chunk_count} chunks) ---\")\n",
    "                \n",
    "                for i, label in enumerate(self.labels):\n",
    "                    if label == cluster_id:\n",
    "                        preview = self.chunks[i][:200] + ('...' if len(self.chunks[i]) > 200 else '')\n",
    "                        print(f\"Chunk {i+1}: {preview}\\n\")\n",
    "            \n",
    "            return self.labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during clustering: {e}\")\n",
    "            self.labels = [-1] * len(self.chunks)\n",
    "            return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a02576",
   "metadata": {},
   "source": [
    "### Entry point to run steps 1 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e1c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname):\n",
    "    if fname in get_processed_files():\n",
    "        print(f\"Skipping {fname} (already processed).\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing new file: {fname}\")\n",
    "    # 1. Read text\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 2. Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)\n",
    "\n",
    "    # 3. Initialize vector manager\n",
    "    vector_manager = VectorStoreManager(PERSIST_DIR, embeddings)\n",
    "    vector_manager.add_chunks(chunks, metadata)\n",
    "\n",
    "    # 4. Mark file as processed \n",
    "    with open(LOG_PATH, \"a\") as log:\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    print(f\"\\nAdded {len(chunks)} chunks from {fname} to Chroma DB.\")\n",
    "\n",
    "    # 5. Cluster chunks from new file\n",
    "    print(f\"\\n--- Starting Clustering Process for '{fname}' ---\")\n",
    "\n",
    "    # Retrieve the chunks AND embeddings for the file just processed\n",
    "    new_chunks, new_embeddings = vector_manager.get_chunks_with_embeddings(fname)\n",
    "\n",
    "    \n",
    "    if new_chunks and new_embeddings and a.any(len(emb) > 0 for emb in new_embeddings):\n",
    "        # Pass the chunks and their pre-computed embeddings to the clusterizer\n",
    "        clusterizer = ChunkClusterizer(new_chunks, new_embeddings)\n",
    "        clusterizer.cluster_chunks()\n",
    "    else:\n",
    "        print(f\"Could not retrieve chunks for {fname} to cluster.\")    \n",
    "    \n",
    "\n",
    "\n",
    "def process_all_new():\n",
    "    new_files = get_new_files()\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files to process: {new_files}\")\n",
    "    \n",
    "    for f in new_files:\n",
    "        process_file(f)\n",
    "    print(\"\\nAll new files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0211580",
   "metadata": {},
   "source": [
    "### Option 1: Process all new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee0cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 new files to process: ['note-3-dt-r-ul.txt', 'note-1-dt-r.txt', 'note-2-ul.txt']\n",
      "\n",
      "Processing new file: note-3-dt-r-ul.txt\n",
      "\n",
      "Chunk 1 (26 tokens):\n",
      "lecture today was fast af. started with trees. entropy vs gini impurity — diff metrics to decide best split.\n",
      "\n",
      "Chunk 2 (14 tokens):\n",
      "both OK. CART = binary tree = each node has 2 splits.\n",
      "\n",
      "Chunk 3 (6 tokens):\n",
      "sklearn uses this.\n",
      "\n",
      "Chunk 4 (47 tokens):\n",
      "code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
      "trees prone to overfit — esp if depth unbounded.\n",
      "\n",
      "Chunk 5 (8 tokens):\n",
      "pruning = way to fix.\n",
      "\n",
      "Chunk 6 (8 tokens):\n",
      "early stopping or post-prune.\n",
      "\n",
      "Chunk 7 (6 tokens):\n",
      "bagging helps = ensemble.\n",
      "\n",
      "Chunk 8 (19 tokens):\n",
      "RandomForest = multiple trees on bootstrapped samples + rand subset of features per split.\n",
      "\n",
      "Chunk 9 (3 tokens):\n",
      "reduces variance.\n",
      "\n",
      "Chunk 10 (12 tokens):\n",
      "trees = interpretability good, but unstable to data changes.\n",
      "\n",
      "Chunk 11 (6 tokens):\n",
      "then prof jumped into regression.\n",
      "\n",
      "Chunk 12 (7 tokens):\n",
      "regularization = shrink model capacity.\n",
      "\n",
      "Chunk 13 (14 tokens):\n",
      "Ridge = L2 norm = λ * Σ(w²).\n",
      "\n",
      "Chunk 14 (14 tokens):\n",
      "Lasso = L1 norm = λ * Σ|w|.\n",
      "\n",
      "Chunk 15 (22 tokens):\n",
      "Ridge keeps all weights ≠ 0, Lasso can zero out → sparse. Lasso good for feature selection.\n",
      "\n",
      "Chunk 16 (71 tokens):\n",
      "ElasticNet = mix of both — good if features correlated\n",
      "code ex:\n",
      "from sklearn.linear_model import ElasticNet model = ElasticNet(alpha=0.1, l1_ratio=0.5) model.fit(X, y)\n",
      "important: scale features before fitting regularized models — otherwise magnitudes skew the penalty.\n",
      "\n",
      "Chunk 17 (77 tokens):\n",
      "StandardScaler or RobustScaler if outliers. tune α via cross-val — use GridSearchCV or RandomizedSearchCV. metrics: RMSE, R². underfit vs overfit — regularization helps balance bias/var tradeoff. last part was unsupervised learning. clustering w/o labels.\n",
      "\n",
      "Chunk 18 (7 tokens):\n",
      "k-means = most used.\n",
      "\n",
      "Chunk 19 (71 tokens):\n",
      "init centers, assign pts, recalc, repeat. problem: sensitive to init. use k-means++. elbow method not always clear. silhouette score better maybe. PCA + k-means often combined for vis. t-SNE only for viz — not for modeling. clusters in t-SNE are sometimes fake.\n",
      "\n",
      "Chunk 20 (7 tokens):\n",
      "DBSCAN = cluster via density.\n",
      "\n",
      "Chunk 21 (19 tokens):\n",
      "can detect noise. great for shape-agnostic clusters. params hard to tune tho.\n",
      "\n",
      "Chunk 22 (7 tokens):\n",
      "hierarchical = dendrograms.\n",
      "\n",
      "Chunk 23 (30 tokens):\n",
      "use ‘ward’ linkage. but slow w/ big data. general note: sklearn models consistent API — fit / predict / score.\n",
      "\n",
      "Chunk 24 (47 tokens):\n",
      "also: why trees don’t need scaling? bcz splits based on order not value. contrast w/ reg models. each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.\n",
      "\n",
      "-> No existing Chroma DB found. A new one will be created when chunks are added.\n",
      "\n",
      "-> Creating new Chroma DB at ./data/chroma_db\n",
      "\n",
      "Added 24 chunks from note-3-dt-r-ul.txt to Chroma DB.\n",
      "\n",
      "--- Starting Clustering Process for 'note-3-dt-r-ul.txt' ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_all_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m, in \u001b[0;36mprocess_all_new\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new files to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m new_files:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll new files processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Retrieve the chunks AND embeddings for the file just processed\u001b[39;00m\n\u001b[1;32m     28\u001b[0m new_chunks, new_embeddings \u001b[38;5;241m=\u001b[39m vector_manager\u001b[38;5;241m.\u001b[39mget_chunks_with_embeddings(fname)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_chunks \u001b[38;5;129;01mand\u001b[39;00m new_embeddings \u001b[38;5;129;01mand\u001b[39;00m a\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mlen\u001b[39m(emb) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m new_embeddings):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Pass the chunks and their pre-computed embeddings to the clusterizer\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     clusterizer \u001b[38;5;241m=\u001b[39m ChunkClusterizer(new_chunks, new_embeddings)\n\u001b[1;32m     34\u001b[0m     clusterizer\u001b[38;5;241m.\u001b[39mcluster_chunks()\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "process_all_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508ab95",
   "metadata": {},
   "source": [
    "### Option 2: Process 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\"note-1-dt-r.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99430c",
   "metadata": {},
   "source": [
    "## Continue pipeline...\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Label clusters with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Rewrite selected outputs with [llama 3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b398a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    7. Store each sub-document in correspondent label as a new file\n",
    "#      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.    8. Add more documents in bulk and generate insights for simple EDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
