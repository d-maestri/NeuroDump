{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a2c20e",
   "metadata": {},
   "source": [
    "# Final Project: NeuroDump - thoughts organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3894bd",
   "metadata": {},
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ea34",
   "metadata": {},
   "source": [
    "### 1.1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0500ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt --- UPDATE THIS AT THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99bb",
   "metadata": {},
   "source": [
    "### 1.2. Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb28ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e9b531c",
   "metadata": {},
   "source": [
    "### 1.3. Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229a806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca80c63",
   "metadata": {},
   "source": [
    "# ### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb99a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52630bd8",
   "metadata": {},
   "source": [
    "# 2. Craft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1fd3a",
   "metadata": {},
   "source": [
    "   1. Input unstructured document file\n",
    "      1. Config paths\n",
    "      2. Check if the input is already processed or is a new file\n",
    "      3. Only continue if it's a new file\n",
    "   2. Split document into chunks of tokens\n",
    "   3. Embed all chunks\n",
    "      1. sentence-transformers (all-MiniLM-L6-v2)\n",
    "      2. save embeddings into Chroba DB\n",
    "      3. keep metada to track original document file for each chunk\n",
    "   4. Cluster the embeddings by semantic similarity (based on all existing vectors in Chroma DB, not only from new file)\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa5ac8",
   "metadata": {},
   "source": [
    "### 1. Input unstructured document\n",
    "Incremental version. It will process only new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d133cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config paths\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          #files\n",
    "LOG_PATH = \"./data/processed_files.txt\"     #list of processed files\n",
    "PERSIST_DIR = \"./data/chroma_db\"            #embeddings database (store vectorized chunks)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7bdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5572a0a",
   "metadata": {},
   "source": [
    "#### 1.2. Check if the input is already processed or is a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9227724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files():\n",
    "    \"\"\"Load list of already processed files from log.\"\"\"\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_new_files():\n",
    "    \"\"\"Get list of new .txt files that haven't been processed yet.\"\"\"\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        return []\n",
    "    \n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c3aed",
   "metadata": {},
   "source": [
    "### 2. Split document into chunks of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafefc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using the embedding model's tokenizer.\"\"\"\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    \"\"\"Detect if a sentence contains code-like patterns.\"\"\"\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords) or\n",
    "        any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "\n",
    "# ### CHUNKING LOGIC\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    \"\"\"Split text into semantic chunks with token limits.\"\"\"\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())                    # Split by double newlines\n",
    "    chunks, current_chunk, current_tokens = [], [], 0                  # Initialize chunking variables\n",
    "    metadata = []                                                      # Store metadata for each chunk\n",
    "\n",
    "    for block in raw_blocks:                                           # Process each text block\n",
    "        sentences = nltk.sent_tokenize(block.strip())                  # Split block into sentences\n",
    "\n",
    "        for sentence in sentences:                                     # Process each sentence\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)                            # Count tokens in sentence\n",
    "\n",
    "            # Handle code-like sentences separately\n",
    "            if is_code_like(sentence):                                 # Check if sentence contains code\n",
    "                if current_chunk:                                      # Save current chunk if exists\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)                                # Add code as separate chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            # Handle conversation transitions\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:                                      # Break chunk at transition words\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            # Check if adding sentence would exceed max tokens\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))                # Save current chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0                 # Reset for new chunk\n",
    "\n",
    "            current_chunk.append(sentence)                            # Add sentence to current chunk\n",
    "            current_tokens += tokens                                  # Update token count\n",
    "\n",
    "        # Save any remaining chunk after block processing\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    if inspect:                                                       # Optional: print chunks for debugging\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d382bde",
   "metadata": {},
   "source": [
    "### 3. Embed all chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c2be1",
   "metadata": {},
   "source": [
    "#### 3.1. Initialize sentence-transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b95d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15a414",
   "metadata": {},
   "source": [
    "#### 3.2. Save embeddings into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VECTOR STORE MANAGEMENT\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, persist_dir, embeddings):\n",
    "        self.persist_dir = persist_dir                                # Store database directory path\n",
    "        self.embeddings = embeddings                                  # Store embedding model\n",
    "        self.vectorstore = self._load_existing()                     # Load existing DB or set to None\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Load existing Chroma database if it exists.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing Chroma DB: {e}\")\n",
    "        \n",
    "        print(\"No existing Chroma DB found. Will create new one.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Add text chunks to vector store with embeddings.\"\"\"\n",
    "        if not self.vectorstore:                                     # Create new database if none exists\n",
    "            print(f\"Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"\n",
    "            )\n",
    "        else:                                                        # Add to existing database\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieve chunks and embeddings for a specific file.\"\"\"\n",
    "        results = self.vectorstore.get(                              # Query database for file chunks\n",
    "            where={\"source\": fname},\n",
    "            include=[\"documents\", \"embeddings\"]\n",
    "        )\n",
    "        \n",
    "        chunks = results.get('documents', [])                       # Extract text chunks\n",
    "        embeddings_list = results.get('embeddings', [])             # Extract embeddings\n",
    "        \n",
    "        # Fallback: compute embeddings if not stored\n",
    "        if not embeddings_list:                                      # Generate embeddings if missing\n",
    "            print(f\"Computing embeddings for {fname}...\")\n",
    "            embeddings_list = [self.embeddings.embed_query(chunk) for chunk in chunks]\n",
    "        \n",
    "        return chunks, embeddings_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecb217",
   "metadata": {},
   "source": [
    "### 4. Cluster the embeddings by semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b7c443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### CLUSTERING LOGIC\n",
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings):\n",
    "        self.chunks = chunks                                         # Store text chunks\n",
    "        self.embeddings = np.array(chunk_embeddings)                 # Convert embeddings to numpy array for ML operations\n",
    "        self.labels = []                                             # Initialize cluster labels\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        \"\"\"Cluster chunks using HDBSCAN algorithm.\"\"\"\n",
    "        print(\"Clustering chunks with HDBSCAN...\")\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if len(self.chunks) < min_cluster_size:                      # Not enough chunks to cluster\n",
    "            print(f\"Not enough chunks to cluster ({len(self.chunks)} < {min_cluster_size})\")\n",
    "            self.labels = [-1] * len(self.chunks)                   # Assign all to noise\n",
    "            return self.labels\n",
    "\n",
    "        # Ensure embeddings are 2D\n",
    "        if self.embeddings.ndim == 1:                               # Reshape 1D embeddings to 2D\n",
    "            self.embeddings = self.embeddings.reshape(1, -1)\n",
    "            \n",
    "        print(f\"Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "\n",
    "        # Perform clustering\n",
    "        clusterizer = hdbscan.HDBSCAN(                                     # Initialize HDBSCAN clustering algorithm\n",
    "            min_cluster_size=min_cluster_size,                             # Minimum number of points in a cluster\n",
    "            min_samples=min_samples,                                       # Minimum samples in neighborhood for core point\n",
    "            metric='euclidean'                                             # Distance metric for clustering\n",
    "        )\n",
    "        self.labels = clusterizer.fit_predict(self.embeddings)             # Fit model and predict cluster labels = generate cluster labels\n",
    "\n",
    "        # Display results\n",
    "        num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)  # Count clusters (exclude noise)\n",
    "        print(f\"Found {num_clusters} clusters\")\n",
    "        print(f\"Cluster distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "            \n",
    "        # Show cluster contents\n",
    "        for cluster_id in sorted(set(self.labels)):                 # Iterate through each cluster\n",
    "            cluster_name = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "            chunk_count = sum(1 for label in self.labels if label == cluster_id)\n",
    "            print(f\"\\n--- {cluster_name} ({chunk_count} chunks) ---\")\n",
    "            \n",
    "            for i, label in enumerate(self.labels):                 # Show chunks in this cluster\n",
    "                if label == cluster_id:\n",
    "                    preview = self.chunks[i][:200] + ('...' if len(self.chunks[i]) > 200 else '')\n",
    "                    print(f\"Chunk {i+1}: {preview}\\n\")\n",
    "            \n",
    "        return self.labels                                                # Return cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a02576",
   "metadata": {},
   "source": [
    "### PROCESSING PIPELINE - steps 1 to 4 for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e1c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### MAIN PROCESSING PIPELINE\n",
    "def process_file(fname):\n",
    "    \"\"\"Process a single file through the complete pipeline.\"\"\"\n",
    "    print(f\"\\n=== Processing file: {fname} ===\")\n",
    "\n",
    "    # Step 1: Read file\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Step 2: Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)        # Split text into chunks\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "    # Step 3: Store in vector database\n",
    "    vector_manager = VectorStoreManager(PERSIST_DIR, embeddings)    # Initialize vector store\n",
    "    vector_manager.add_chunks(chunks, metadata)                     # Add chunks with embeddings\n",
    "    print(f\"Stored {len(chunks)} chunks in vector database\")\n",
    "    \n",
    "    # Step 4: Mark as processed\n",
    "    with open(LOG_PATH, \"a\") as log:                                # Log processed file\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    # Step 5: Cluster chunks\n",
    "    print(f\"\\n--- Clustering chunks from {fname} ---\")\n",
    "    new_chunks, new_embeddings = vector_manager.get_chunks_with_embeddings(fname)  # Retrieve stored data\n",
    "    \n",
    "    clusterizer = ChunkClusterizer(new_chunks, new_embeddings)      # Initialize clustering\n",
    "    clusterizer.cluster_chunks()                                    # Perform clustering\n",
    "    \n",
    "\n",
    "\n",
    "def process_all_new():\n",
    "    \"\"\"Process all new files in the input directory.\"\"\"\n",
    "    new_files = get_new_files()                                     # Get list of unprocessed files\n",
    "    \n",
    "    if not new_files:                                               # Exit early if no new files\n",
    "        print(\"No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files: {new_files}\")\n",
    "    \n",
    "    for fname in new_files:                                         # Process each new file\n",
    "        process_file(fname)\n",
    "    \n",
    "    print(f\"\\n=== Completed processing {len(new_files)} files ===\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FOR LATER: \"MAIN GUARD\" OR \"ENTRY POINT PATTERN\" \n",
    "# EXECUTION \n",
    "#if __name__ == \"__main__\":\n",
    "#    process_all_new()                                               # Run the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0211580",
   "metadata": {},
   "source": [
    "### Option 1: Process all new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee0cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 new files: ['note-3-dt-r-ul.txt', 'note-1-dt-r.txt', 'note-2-ul.txt']\n",
      "\n",
      "=== Processing file: note-3-dt-r-ul.txt ===\n",
      "\n",
      "Chunk 1 (26 tokens):\n",
      "lecture today was fast af. started with trees. entropy vs gini impurity — diff metrics to decide best split.\n",
      "\n",
      "Chunk 2 (14 tokens):\n",
      "both OK. CART = binary tree = each node has 2 splits.\n",
      "\n",
      "Chunk 3 (6 tokens):\n",
      "sklearn uses this.\n",
      "\n",
      "Chunk 4 (47 tokens):\n",
      "code: from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
      "trees prone to overfit — esp if depth unbounded.\n",
      "\n",
      "Chunk 5 (8 tokens):\n",
      "pruning = way to fix.\n",
      "\n",
      "Chunk 6 (8 tokens):\n",
      "early stopping or post-prune.\n",
      "\n",
      "Chunk 7 (6 tokens):\n",
      "bagging helps = ensemble.\n",
      "\n",
      "Chunk 8 (19 tokens):\n",
      "RandomForest = multiple trees on bootstrapped samples + rand subset of features per split.\n",
      "\n",
      "Chunk 9 (3 tokens):\n",
      "reduces variance.\n",
      "\n",
      "Chunk 10 (12 tokens):\n",
      "trees = interpretability good, but unstable to data changes.\n",
      "\n",
      "Chunk 11 (6 tokens):\n",
      "then prof jumped into regression.\n",
      "\n",
      "Chunk 12 (7 tokens):\n",
      "regularization = shrink model capacity.\n",
      "\n",
      "Chunk 13 (14 tokens):\n",
      "Ridge = L2 norm = λ * Σ(w²).\n",
      "\n",
      "Chunk 14 (14 tokens):\n",
      "Lasso = L1 norm = λ * Σ|w|.\n",
      "\n",
      "Chunk 15 (22 tokens):\n",
      "Ridge keeps all weights ≠ 0, Lasso can zero out → sparse. Lasso good for feature selection.\n",
      "\n",
      "Chunk 16 (71 tokens):\n",
      "ElasticNet = mix of both — good if features correlated\n",
      "code ex:\n",
      "from sklearn.linear_model import ElasticNet model = ElasticNet(alpha=0.1, l1_ratio=0.5) model.fit(X, y)\n",
      "important: scale features before fitting regularized models — otherwise magnitudes skew the penalty.\n",
      "\n",
      "Chunk 17 (77 tokens):\n",
      "StandardScaler or RobustScaler if outliers. tune α via cross-val — use GridSearchCV or RandomizedSearchCV. metrics: RMSE, R². underfit vs overfit — regularization helps balance bias/var tradeoff. last part was unsupervised learning. clustering w/o labels.\n",
      "\n",
      "Chunk 18 (7 tokens):\n",
      "k-means = most used.\n",
      "\n",
      "Chunk 19 (71 tokens):\n",
      "init centers, assign pts, recalc, repeat. problem: sensitive to init. use k-means++. elbow method not always clear. silhouette score better maybe. PCA + k-means often combined for vis. t-SNE only for viz — not for modeling. clusters in t-SNE are sometimes fake.\n",
      "\n",
      "Chunk 20 (7 tokens):\n",
      "DBSCAN = cluster via density.\n",
      "\n",
      "Chunk 21 (19 tokens):\n",
      "can detect noise. great for shape-agnostic clusters. params hard to tune tho.\n",
      "\n",
      "Chunk 22 (7 tokens):\n",
      "hierarchical = dendrograms.\n",
      "\n",
      "Chunk 23 (30 tokens):\n",
      "use ‘ward’ linkage. but slow w/ big data. general note: sklearn models consistent API — fit / predict / score.\n",
      "\n",
      "Chunk 24 (47 tokens):\n",
      "also: why trees don’t need scaling? bcz splits based on order not value. contrast w/ reg models. each algo has tradeoffs — no free lunch! choose based on data, task, interpretability needs.\n",
      "Created 24 chunks\n",
      "No existing Chroma DB found. Will create new one.\n",
      "Creating new Chroma DB at ./data/chroma_db\n",
      "Stored 24 chunks in vector database\n",
      "\n",
      "--- Clustering chunks from note-3-dt-r-ul.txt ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_all_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mprocess_all_new\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m new_files:                                         \u001b[38;5;66;03m# Process each new file\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Completed processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 5: Cluster chunks\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Clustering chunks from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m new_chunks, new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mvector_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunks_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Retrieve stored data\u001b[39;00m\n\u001b[1;32m     27\u001b[0m clusterizer \u001b[38;5;241m=\u001b[39m ChunkClusterizer(new_chunks, new_embeddings)      \u001b[38;5;66;03m# Initialize clustering\u001b[39;00m\n\u001b[1;32m     28\u001b[0m clusterizer\u001b[38;5;241m.\u001b[39mcluster_chunks()\n",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36mVectorStoreManager.get_chunks_with_embeddings\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     46\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m, [])             \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Fallback: compute embeddings if not stored\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embeddings_list:                                      \u001b[38;5;66;03m# Generate embeddings if missing\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     embeddings_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39membed_query(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "process_all_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508ab95",
   "metadata": {},
   "source": [
    "### Option 2: Process 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\"note-1-dt-r.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99430c",
   "metadata": {},
   "source": [
    "## Continue pipeline...\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Label clusters with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Rewrite selected outputs with [llama 3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b398a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    7. Store each sub-document in correspondent label as a new file\n",
    "#      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.    8. Add more documents in bulk and generate insights for simple EDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
