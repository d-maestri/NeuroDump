{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a2c20e",
   "metadata": {},
   "source": [
    "# Final Project: NeuroDump - thoughts organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3894bd",
   "metadata": {},
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ea34",
   "metadata": {},
   "source": [
    "### 1.1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0500ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt --- UPDATE THIS AT THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99bb",
   "metadata": {},
   "source": [
    "### 1.2. Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb28ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e9b531c",
   "metadata": {},
   "source": [
    "### 1.3. Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229a806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca80c63",
   "metadata": {},
   "source": [
    "# ### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb99a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "\n",
    "nltk.download('punkt')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52630bd8",
   "metadata": {},
   "source": [
    "# 2. Craft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1fd3a",
   "metadata": {},
   "source": [
    "   1. Input unstructured document file\n",
    "      1. Config paths\n",
    "      2. Check if the input is already processed or is a new file\n",
    "      3. Only continue if it's a new file\n",
    "   2. Split document into chunks of tokens\n",
    "   3. Embed all chunks\n",
    "      1. sentence-transformers (all-MiniLM-L6-v2)\n",
    "      2. save embeddings into Chroba DB\n",
    "      3. keep metada to track original document file for each chunk\n",
    "   4. Cluster the embeddings by semantic similarity (based on all existing vectors in Chroma DB, not only from new file)\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa5ac8",
   "metadata": {},
   "source": [
    "### 1. Input unstructured document\n",
    "Incremental version. It will process only new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d133cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config paths\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          #files\n",
    "LOG_PATH = \"./data/processed_files.txt\"     #list of processed files\n",
    "PERSIST_DIR = \"./data/chroma_db\"            #embeddings database (store vectorized chunks)\n",
    "CLUSTER_DIR = \"./data/cluster_dir/\"         #clusters database\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "os.makedirs(CLUSTER_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7bdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "HF_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5572a0a",
   "metadata": {},
   "source": [
    "#### 1.2. Check if the input is already processed or is a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9227724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files():\n",
    "    \"\"\"Load list of already processed files from log.\"\"\"\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_new_files():\n",
    "    \"\"\"Get list of new .txt files that haven't been processed yet.\"\"\"\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        return []\n",
    "    \n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c3aed",
   "metadata": {},
   "source": [
    "### 2. Split document into chunks of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafefc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using the embedding model's tokenizer.\"\"\"\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    \"\"\"Detect if a sentence contains code-like patterns.\"\"\"\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords) or\n",
    "        any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "\n",
    "# ### CHUNKING LOGIC\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    \"\"\"Split text into semantic chunks with token limits.\"\"\"\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())                    # Split by double newlines\n",
    "    chunks, current_chunk, current_tokens = [], [], 0                  # Initialize chunking variables\n",
    "    metadata = []                                                      # Store metadata for each chunk\n",
    "\n",
    "    for block in raw_blocks:                                           # Process each text block\n",
    "        sentences = nltk.sent_tokenize(block.strip())                  # Split block into sentences\n",
    "\n",
    "        for sentence in sentences:                                     # Process each sentence\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)                            # Count tokens in sentence\n",
    "\n",
    "            # Handle code-like sentences separately\n",
    "            if is_code_like(sentence):                                 # Check if sentence contains code\n",
    "                if current_chunk:                                      # Save current chunk if exists\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)                                # Add code as separate chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            # Handle conversation transitions\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:                                      # Break chunk at transition words\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            # Check if adding sentence would exceed max tokens\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))                # Save current chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0                 # Reset for new chunk\n",
    "\n",
    "            current_chunk.append(sentence)                            # Add sentence to current chunk\n",
    "            current_tokens += tokens                                  # Update token count\n",
    "\n",
    "        # Save any remaining chunk after block processing\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    # if inspect:                                                       # Optional: print chunks for debugging\n",
    "    #     for i, chunk in enumerate(chunks):\n",
    "    #         print(f\"\\n=== Chunks ===\")\n",
    "    #         print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d382bde",
   "metadata": {},
   "source": [
    "### 3. Embed all chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15a414",
   "metadata": {},
   "source": [
    "#### 3.1. Save embeddings into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VECTOR STORE MANAGEMENT\n",
    "class VectorStoreManager:\n",
    "    def __init__(self):\n",
    "        self.persist_dir = PERSIST_DIR                              # Store database directory path\n",
    "        self.embeddings = HF_embeddings                          # Store embedding model\n",
    "        self.vectorstore = self._load_existing()                    # Load existing DB or set to None\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Load existing Chroma database if it exists.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"\\n=== Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n=== Error loading existing Chroma DB: {e}\")\n",
    "        \n",
    "        print(\"\\n=== No existing Chroma DB found. Will create new one.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Add text chunks to vector store with embeddings.\"\"\"\n",
    "        if not self.vectorstore:                                     # Create new database if none exists\n",
    "            print(f\"\\n=== Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"\n",
    "            )\n",
    "        else:                                                        # Add to existing database\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "\n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieve chunks, embeddings, and metadata for a specific file.\"\"\"\n",
    "        results = self.vectorstore.get(\n",
    "            where={\"source\": fname},\n",
    "            include=[\"documents\", \"embeddings\", \"metadatas\"]\n",
    "        )\n",
    "        chunks = results.get('documents', [])               # Extract text chunks\n",
    "        embeddings_list = results.get('embeddings', [])     # Exttact embeddings\n",
    "        metadatas = results.get('metadatas', [])            # Extract metadata\n",
    "    \n",
    "        return chunks, embeddings_list, metadatas        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecb217",
   "metadata": {},
   "source": [
    "### 4. Cluster the embeddings by semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b53ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### CLUSTERING LOGIC\n",
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings, metadata, fname):\n",
    "        self.chunks = chunks                                         # Store text chunks\n",
    "        self.embeddings = np.array(chunk_embeddings)                 # Convert embeddings to numpy array for ML operations\n",
    "        self.metadata = metadata                                     # Save metadata       \n",
    "        self.fname = fname\n",
    "        self.labels = []                                             # Initialize cluster labels\n",
    "        self.clusters = defaultdict(list)                            # cluster_id -> list of dicts {chunk, metadata}\n",
    "\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        \"\"\"Cluster chunks using HDBSCAN algorithm.\"\"\"\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if len(self.chunks) < min_cluster_size:                      # Not enough chunks to cluster\n",
    "            print(f\"\\n=== Not enough chunks to cluster ({len(self.chunks)} < {min_cluster_size}) ===\")\n",
    "            self.labels = [-1] * len(self.chunks)                   # Assign all to noise\n",
    "            return self.labels\n",
    "\n",
    "        # Ensure embeddings are 2D\n",
    "        if self.embeddings.ndim == 1:                               # Reshape 1D embeddings to 2D\n",
    "            self.embeddings = self.embeddings.reshape(1, -1)\n",
    "            \n",
    "        #print(f\"\\n===> Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "\n",
    "        # Perform clustering\n",
    "        clusterizer = hdbscan.HDBSCAN(                                     # Initialize HDBSCAN clustering algorithm\n",
    "            min_cluster_size=min_cluster_size,                             # Minimum number of points in a cluster\n",
    "            min_samples=min_samples,                                       # Minimum samples in neighborhood for core point\n",
    "            metric='euclidean'                                             # Distance metric for clustering\n",
    "        )\n",
    "        self.labels = clusterizer.fit_predict(self.embeddings)             # Fit model and predict cluster labels = generate cluster labels\n",
    "\n",
    "        # Display results\n",
    "        num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)  # Count clusters (exclude noise)\n",
    "        print(f\"\\n=== Found {num_clusters} clusters\")\n",
    "        print(\"\\n=== Cluster distribution ===\")\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            cname = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "            print(f\"{cname}: {count} chunks\")    \n",
    "                    \n",
    "        # Show cluster contents\n",
    "        for cluster_id in sorted(set(self.labels)):\n",
    "            cluster_name = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "            print(f\"\\n=== {cluster_name} ===\")\n",
    "\n",
    "            for i, label in enumerate(self.labels):\n",
    "                if label == cluster_id:\n",
    "                    # Save the chunk into the cluster dictionary with global ID\n",
    "                    self.clusters[str(cluster_id)].append({\n",
    "                        \"chunk_id\": i,                      # Global ID\n",
    "                        \"chunk\": self.chunks[i],\n",
    "                        \"metadata\": self.metadata[i]\n",
    "                    })\n",
    "\n",
    "                    # Display clearly: Chunk [global id]\n",
    "                    display(Markdown(f\"**Chunk {i}:** {self.chunks[i]}\"))\n",
    "            \n",
    "        return self.labels                                   # Return cluster labels\n",
    "        \n",
    "    def save_clusters(self):\n",
    "        \"\"\"Save clusters with metadata\"\"\"\n",
    "        out_path = os.path.join(CLUSTER_DIR, self.fname.replace(\".txt\", \"_clusters.json\"))\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.clusters, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n=== Saved clusters to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a02576",
   "metadata": {},
   "source": [
    "### PROCESSING PIPELINE - steps 1 to 4 for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e1c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### MAIN PROCESSING PIPELINE\n",
    "def process_file(fname):\n",
    "    \"\"\"Process a single file through the complete pipeline.\"\"\"\n",
    "    print(f\"\\n\\n=============== PROCESSING FILE: {fname} ===============\")\n",
    "\n",
    "    # Step 1: Read file\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Step 2: Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)        # Split text into chunks\n",
    "    print(f\"\\n=== Created {len(chunks)} chunks\")\n",
    "\n",
    "    # Step 3: Store in vector database\n",
    "    vector_manager = VectorStoreManager()    # Initialize vector store\n",
    "    vector_manager.add_chunks(chunks, metadata)                     # Add chunks with embeddings\n",
    "    print(f\"\\n=== Stored {len(chunks)} chunks in Chroma DB\")\n",
    "    \n",
    "    # Step 4: Mark as processed\n",
    "    with open(LOG_PATH, \"a\") as log:                                # Log processed file\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    # Step 5: Cluster chunks\n",
    "    print(f\"\\n=== CLUSTERING CHUNKS FROM {fname}\")\n",
    "    chunks, embeddings, metadatas = vector_manager.get_chunks_with_embeddings(fname) # Retrieve stored data\n",
    "\n",
    "    clusterizer = ChunkClusterizer(chunks, embeddings, metadatas, fname)             # Initialize clustering\n",
    "    clusterizer.cluster_chunks()                                            # Perform clustering\n",
    "    clusterizer.save_clusters()                                             # Save clusters\n",
    "\n",
    "\n",
    "def process_all_new():\n",
    "    \"\"\"Process all new files in the input directory.\"\"\"\n",
    "    new_files = get_new_files()                                     # Get list of unprocessed files\n",
    "    \n",
    "    if not new_files:                                               # Exit early if no new files\n",
    "        print(\"\\n=== No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Found {len(new_files)} new files: {new_files}\")\n",
    "    \n",
    "    for fname in new_files:                                         # Process each new file\n",
    "        process_file(fname)\n",
    "    \n",
    "    print(f\"\\n=== Completed processing {len(new_files)} files\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FOR LATER: \"MAIN GUARD\" OR \"ENTRY POINT PATTERN\" \n",
    "# EXECUTION \n",
    "#if __name__ == \"__main__\":\n",
    "#    process_all_new()                                               # Run the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0211580",
   "metadata": {},
   "source": [
    "### Option 1: Process all new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee0cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== No new files to process.\n"
     ]
    }
   ],
   "source": [
    "process_all_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508ab95",
   "metadata": {},
   "source": [
    "### Option 2: Process 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700cd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\"note-1-dt-r.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99430c",
   "metadata": {},
   "source": [
    "## Continue pipeline...\n",
    "   5. Label each cluster based on existing labels in Chroma DB, if no match, create a new label\n",
    "      1. maybe prompt to LLM to generate labels?\n",
    "   6. Rewrite each labeled-cluster to connect all the chunks and create a coherent sub-document \n",
    "      1. use LLM for that.\n",
    "      2. output should be like: sub-document 1 (label 1), sub-document 2 (label 2)\n",
    "   7. Store each sub-document in correspondent label as a new file\n",
    "      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n",
    "      2. Use Notion API, the sub-document should be a new file in a Notion database, with the correpondent Label and metadata\n",
    "   8. Add more documents in bulk and generate insights for simple EDA\n",
    "       1.  Number of labels, sub-documents in each label, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5665d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Label clusters with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2fb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(model: str, prompt: str) -> str:\n",
    "    process = subprocess.Popen(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True  # so we use strings, not bytes\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=prompt)\n",
    "    if stderr:\n",
    "        print(\"OLLAMA STDERR:\", stderr)\n",
    "    return stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02965bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a smart assistant helping organize fragmented personal notes.\n",
    "\n",
    "The notes were previously split into chunks and clustered semantically. Each original file has its own clusters, saved as JSON files inside: ./data/cluster_dir/. Your task is to refine those clusters and rewrite them as organized sub-notes.\n",
    "\n",
    "1. REORGANIZE CLUSTERS PER FILE:\n",
    "- You may SPLIT or MERGE clusters WITHIN EACH FILE to better reflect topic boundaries.\n",
    "- You may RENAME clusters with meaningful TOPIC NAMES.\n",
    "- While clusters are per file, topics should be GLOBALLY CONSISTENT across files.\n",
    "  Example: cluster_0 from note-1.txt and cluster_0 from note-2.txt might both refer to the same topic \"decision trees\".\n",
    "\n",
    "2. GENERATE CLUSTER NAMES:\n",
    "- For each cluster ID (like cluster_0, cluster_1...), assign a clear, short TOPIC NAME.\n",
    "- Save the cluster-name mapping as a DICTIONARY:\n",
    "  {\n",
    "    \"cluster_0\": \"decision trees\",\n",
    "    \"cluster_1\": \"regularization\",\n",
    "    ...\n",
    "  }\n",
    "\n",
    "3. REWRITE EACH CLUSTER:\n",
    "- Combine the CHUNKS within each cluster into a COHERENT NOTE.\n",
    "- The output should be CLEAR, READABLE, and preserve the original tone.\n",
    "- You may REPHRASE, REORDER, or EDIT content to improve clarity.\n",
    "- You MUST preserve the original content. DO NOT add new content. Focus only on text clarity.\n",
    "\n",
    "4. SAVE RESULTS:\n",
    "- Each rewritten cluster note must be saved to:\n",
    "  ./data/outputs/rewritten_notes/{TOPIC_NAME}/\n",
    "    └── {ORIGINAL_FILE}_cluster-{ID}.txt\n",
    "\n",
    "  Example:\n",
    "  ./data/outputs/rewritten_notes/decision_trees/note-1_cluster-0.txt\n",
    "\n",
    "- Save the CLUSTER_NAMES dictionary to:\n",
    "  ./data/outputs/cluster_names.json\n",
    "\n",
    "- Save any MERGE, SPLIT, or REORGANIZATION CHANGES you make per file to:\n",
    "  ./data/outputs/LLM_recluster/{ORIGINAL_FILE}_LLM_recluster.json\n",
    "\n",
    "5. NOTES:\n",
    "- Do NOT include topic names in file names.\n",
    "- All cluster names must be FOLDER-SAFE (no special characters or spaces; use underscores if needed).\n",
    "- Rewritten notes should be based ONLY on CHUNKS FROM THE CORRESPONDING FILE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8eae879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA STDERR: \u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\n",
      " Hello! How can I assist you today?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Say hello\"\n",
    "print(query_ollama(\"mistral\", test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e0de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA STDERR: \u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?25l\u001b[?25h\n",
      " To complete this task, I'll follow the steps you've outlined:\n",
      "\n",
      "1. REORGANIZE CLUSTERS PER FILE:\n",
      "   - Load all clusters from each JSON file in ./data/cluster_dir/\n",
      "   - Review and analyze clusters to identify overlapping or related topics, then group them accordingly while considering topic boundaries\n",
      "   - Rename the clusters with meaningful topic names that reflect their content, making sure they are globally consistent across files.\n",
      "   - Save any reorganization changes in ./data/outputs/LLM_recluster/{ORIGINAL_FILE}_LLM_recluster.json (e.g., `./data/outputs/LLM_recluster/note-1.txt_LLM_recluster.json`)\n",
      "\n",
      "2. GENERATE CLUSTER NAMES:\n",
      "   - Create a dictionary that maps each cluster ID to a clear, short topic name based on its content. (e.g., `{\"cluster_0\": \"decision trees\", \"cluster_1\": \"regularization\"}`)\n",
      "   - Save the cluster-name mapping as ./data/outputs/cluster_names.json\n",
      "\n",
      "3. REWRITE EACH CLUSTER:\n",
      "   - Combine the chunks within each reorganized cluster into a coherent note, ensuring clarity, readability, and preserving the original tone. This may involve rephrasing, reordering, or editing content.\n",
      "   - Save the rewritten clusters to ./data/outputs/rewritten_notes/{TOPIC_NAME}/ with filenames based on their corresponding original file and cluster ID (e.g., `./data/outputs/rewritten_notes/decision_trees/note-1_cluster-0.txt`)\n",
      "\n",
      "4. SAVE RESULTS:\n",
      "   - The rewritten clusters will be saved as described in step 3.\n",
      "   - The cluster-name dictionary and any reorganization changes per file will be saved as specified in steps 2 and 1, respectively.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_ollama(\"mistral\", SYSTEM_PROMPT)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text, start_key):\n",
    "    # crude way: find the JSON blob starting with start_key, then parse it\n",
    "    pattern = re.compile(rf\"{start_key}\\s*(:|\\=)?\\s*(\\{{.*?\\}})\", re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(2))\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing JSON:\", e)\n",
    "    return None\n",
    "\n",
    "cluster_names = extract_json(llm_response, \"cluster_names\")\n",
    "llm_recluster_changes = extract_json(llm_response, \"reorganization_changes\")\n",
    "rewritten_notes = extract_json(llm_response, \"rewritten_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Rewrite selected outputs with [llama 3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b398a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    7. Store each sub-document in correspondent label as a new file\n",
    "#      1. For test, generate a new folder locally and save the sub-document as .txt file in the folder with the correspondent label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.    8. Add more documents in bulk and generate insights for simple EDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
