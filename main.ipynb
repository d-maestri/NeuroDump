{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a2c20e",
   "metadata": {},
   "source": [
    "# Final Project: NeuroDump - thoughts organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3894bd",
   "metadata": {},
   "source": [
    "## I. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ea34",
   "metadata": {},
   "source": [
    "### 1.1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0500ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt --- UPDATE THIS AT THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99bb",
   "metadata": {},
   "source": [
    "### 1.2. Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "\n",
    "nltk.download('punkt')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b531c",
   "metadata": {},
   "source": [
    "### 1.3. Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d133cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config paths\n",
    "FOLDER_PATH = \"./data/mock_notes/\"          #files\n",
    "LOG_PATH = \"./data/processed_files.txt\"     #list of processed files\n",
    "PERSIST_DIR = \"./data/chroma_db\"            #embeddings database (store vectorized chunks)\n",
    "CLUSTER_DIR = \"./data/cluster_dir/\"         #clusters database\n",
    "SUBNOTES_RAW = \"./data/outputs/subnotes_raw\"  #subnotes raw (chunks list)\n",
    "LLM_RECLUSTER = \"./data/outputs/LLM_recluster\"\n",
    "FINAL_NOTES = \"./data/outputs/final_notes\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "os.makedirs(CLUSTER_DIR, exist_ok=True)\n",
    "os.makedirs(SUBNOTES_RAW, exist_ok=True)\n",
    "os.makedirs(LLM_RECLUSTER, exist_ok=True)\n",
    "os.makedirs(FINAL_NOTES, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "HF_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Save JSON file \n",
    "def save_json(data, folder_path, filename):\n",
    "    \"\"\"Save data as JSON to specified folder and filename. Creates folder if missing.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    out_path = os.path.join(folder_path, filename)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n=== Saved JSON to: {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f084ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_base_filename(filename):\n",
    "    \"\"\"Extract base filename without extension and suffixes.\"\"\"\n",
    "    # Remove .txt extension if present\n",
    "    if filename.endswith('.txt'):\n",
    "        filename = filename[:-4]\n",
    "    \n",
    "    # Remove _clusters suffix if present\n",
    "    if filename.endswith('_clusters'):\n",
    "        filename = filename[:-9]\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2396762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_filename_from_clusters(clusters_file_path):\n",
    "    \"\"\"Extract original filename from clusters file path.\"\"\"\n",
    "    filename = os.path.basename(clusters_file_path)\n",
    "    return extract_base_filename(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52630bd8",
   "metadata": {},
   "source": [
    "# II. Craft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa5ac8",
   "metadata": {},
   "source": [
    "### 1. Input unstructured document\n",
    "Incremental version. It will process only new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9227724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files():\n",
    "    \"\"\"Load list of already processed files from log.\"\"\"\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_new_files():\n",
    "    \"\"\"Get list of new .txt files that haven't been processed yet.\"\"\"\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        return []\n",
    "    \n",
    "    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".txt\")]\n",
    "    processed_files = get_processed_files()\n",
    "    return [f for f in all_files if f not in processed_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c3aed",
   "metadata": {},
   "source": [
    "### 2. Split document into chunks of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafefc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using the embedding model's tokenizer.\"\"\"\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def is_code_like(sentence):\n",
    "    \"\"\"Detect if a sentence contains code-like patterns.\"\"\"\n",
    "    code_keywords = [\"from \", \"import \", \"def \", \"return \", \"class \", \"for \", \"if \", \"else\", \"while \", \"try:\", \"except\", \"print(\"]\n",
    "    code_symbols = [\"=\", \"()\", \"[]\", \"{}\", \"->\", \"::\"]\n",
    "    sentence_lower = sentence.strip().lower()\n",
    "    return (\n",
    "        any(sentence_lower.startswith(k) for k in code_keywords) or\n",
    "        any(sym in sentence for sym in code_symbols)\n",
    "    )\n",
    "\n",
    "\n",
    "# ### CHUNKING LOGIC\n",
    "def chunk_text(text, source_name, min_tokens=40, max_tokens=100, inspect=False):\n",
    "    \"\"\"Split text into semantic chunks with token limits.\"\"\"\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", text.strip())                    # Split by double newlines\n",
    "    chunks, current_chunk, current_tokens = [], [], 0                  # Initialize chunking variables\n",
    "    metadata = []                                                      # Store metadata for each chunk\n",
    "\n",
    "    for block in raw_blocks:                                           # Process each text block\n",
    "        sentences = nltk.sent_tokenize(block.strip())                  # Split block into sentences\n",
    "\n",
    "        for sentence in sentences:                                     # Process each sentence\n",
    "            sentence = sentence.strip()\n",
    "            tokens = count_tokens(sentence)                            # Count tokens in sentence\n",
    "\n",
    "            # Handle code-like sentences separately\n",
    "            if is_code_like(sentence):                                 # Check if sentence contains code\n",
    "                if current_chunk:                                      # Save current chunk if exists\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "                chunks.append(sentence)                                # Add code as separate chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                continue\n",
    "\n",
    "            # Handle conversation transitions\n",
    "            if any(sentence.lower().startswith(w) for w in [\"btw\", \"wait\", \"oh\", \"then\", \"also\", \"now\", \"next\"]):\n",
    "                if current_chunk:                                      # Break chunk at transition words\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    metadata.append({\"source\": source_name})\n",
    "                    current_chunk, current_tokens = [], 0\n",
    "\n",
    "            # Check if adding sentence would exceed max tokens\n",
    "            if current_tokens + tokens > max_tokens and current_tokens >= min_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))                # Save current chunk\n",
    "                metadata.append({\"source\": source_name})\n",
    "                current_chunk, current_tokens = [], 0                 # Reset for new chunk\n",
    "\n",
    "            current_chunk.append(sentence)                            # Add sentence to current chunk\n",
    "            current_tokens += tokens                                  # Update token count\n",
    "\n",
    "        # Save any remaining chunk after block processing\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            metadata.append({\"source\": source_name})\n",
    "            current_chunk, current_tokens = [], 0\n",
    "\n",
    "    # if inspect:                                                       # Optional: print chunks for debugging\n",
    "    #     for i, chunk in enumerate(chunks):\n",
    "    #         print(f\"\\n=== Chunks ===\")\n",
    "    #         print(f\"\\nChunk {i+1} ({count_tokens(chunk)} tokens):\\n{chunk}\")\n",
    "\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d382bde",
   "metadata": {},
   "source": [
    "### 3. Embeddings and save vectors in Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VECTOR STORE MANAGEMENT\n",
    "class VectorStoreManager:\n",
    "    def __init__(self):\n",
    "        self.persist_dir = PERSIST_DIR                              # Store database directory path\n",
    "        self.embeddings = HF_embeddings                          # Store embedding model\n",
    "        self.vectorstore = self._load_existing()                    # Load existing DB or set to None\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"Load existing Chroma database if it exists.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "                print(f\"\\n=== Loading existing Chroma DB from {self.persist_dir}\")\n",
    "                return Chroma(\n",
    "                    persist_directory=self.persist_dir,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    collection_name=\"default\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n=== Error loading existing Chroma DB: {e}\")\n",
    "        \n",
    "        print(\"\\n=== No existing Chroma DB found. Will create new one.\")\n",
    "        return None\n",
    "\n",
    "    def add_chunks(self, chunks, metadata):\n",
    "        \"\"\"Add text chunks to vector store with embeddings.\"\"\"\n",
    "        if not self.vectorstore:                                     # Create new database if none exists\n",
    "            print(f\"\\n=== Creating new Chroma DB at {self.persist_dir}\")\n",
    "            self.vectorstore = Chroma.from_texts(\n",
    "                texts=chunks,\n",
    "                metadatas=metadata,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.persist_dir,\n",
    "                collection_name=\"default\"\n",
    "            )\n",
    "        else:                                                        # Add to existing database\n",
    "            self.vectorstore.add_texts(texts=chunks, metadatas=metadata)\n",
    "\n",
    "\n",
    "    def get_chunks_with_embeddings(self, fname):\n",
    "        \"\"\"Retrieve chunks, embeddings, and metadata for a specific file.\"\"\"\n",
    "        results = self.vectorstore.get(\n",
    "            where={\"source\": fname},\n",
    "            include=[\"documents\", \"embeddings\", \"metadatas\"]\n",
    "        )\n",
    "        chunks = results.get('documents', [])               # Extract text chunks\n",
    "        embeddings_list = results.get('embeddings', [])     # Exttact embeddings\n",
    "        metadatas = results.get('metadatas', [])            # Extract metadata\n",
    "    \n",
    "        return chunks, embeddings_list, metadatas        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecb217",
   "metadata": {},
   "source": [
    "### 4. Clusterize embeddings with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b53ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### CLUSTERING LOGIC\n",
    "class ChunkClusterizer:\n",
    "    def __init__(self, chunks, chunk_embeddings, metadata, fname):\n",
    "        self.chunks = chunks                                         # Store text chunks\n",
    "        self.embeddings = np.array(chunk_embeddings)                 # Convert embeddings to numpy array for ML operations\n",
    "        self.metadata = metadata                                     # Save metadata       \n",
    "        self.fname = fname\n",
    "        self.labels = []                                             # Initialize cluster labels\n",
    "        self.clusters = defaultdict(list)                            # cluster_id -> list of dicts {chunk, metadata}\n",
    "\n",
    "\n",
    "    def cluster_chunks(self, min_cluster_size=2, min_samples=1):\n",
    "        \"\"\"Cluster chunks using HDBSCAN algorithm.\"\"\"\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if len(self.chunks) < min_cluster_size:                      # Not enough chunks to cluster\n",
    "            print(f\"\\n=== Not enough chunks to cluster ({len(self.chunks)} < {min_cluster_size}) ===\")\n",
    "            self.labels = [-1] * len(self.chunks)                   # Assign all to noise\n",
    "            return self.labels\n",
    "\n",
    "        # Ensure embeddings are 2D\n",
    "        if self.embeddings.ndim == 1:                               # Reshape 1D embeddings to 2D\n",
    "            self.embeddings = self.embeddings.reshape(1, -1)\n",
    "            \n",
    "        #print(f\"\\n===> Clustering {len(self.chunks)} chunks with embeddings shape: {self.embeddings.shape}\")\n",
    "            \n",
    "\n",
    "        # Perform clustering\n",
    "        clusterizer = hdbscan.HDBSCAN(                                     # Initialize HDBSCAN clustering algorithm\n",
    "            min_cluster_size=min_cluster_size,                             # Minimum number of points in a cluster\n",
    "            min_samples=min_samples,                                       # Minimum samples in neighborhood for core point\n",
    "            metric='euclidean'                                             # Distance metric for clustering\n",
    "        )\n",
    "        self.labels = clusterizer.fit_predict(self.embeddings)             # Fit model and predict cluster labels = generate cluster labels\n",
    "\n",
    "        # Display results\n",
    "        num_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)  # Count clusters (exclude noise)\n",
    "        print(f\"\\n=== Found {num_clusters} clusters\")\n",
    "        print(\"\\n=== Cluster distribution ===\")\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            cname = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "            print(f\"{cname}: {count} chunks\")    \n",
    "                    \n",
    "        # Show cluster contents\n",
    "        for cluster_id in sorted(set(self.labels)):\n",
    "            cluster_name = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "            print(f\"\\n=== {cluster_name} ===\")\n",
    "\n",
    "            for i, label in enumerate(self.labels):\n",
    "                if label == cluster_id:\n",
    "                    # Save the chunk into the cluster dictionary with global ID\n",
    "                    self.clusters[str(cluster_id)].append({\n",
    "                        \"chunk_id\": i,                      # Global ID\n",
    "                        \"chunk\": self.chunks[i],\n",
    "                        \"metadata\": self.metadata[i]\n",
    "                    })\n",
    "\n",
    "                    # Display clearly: Chunk [global id]\n",
    "                    display(Markdown(f\"**Chunk {i}:** {self.chunks[i]}\"))\n",
    "            \n",
    "        return self.labels                                   # Return cluster labels\n",
    "        \n",
    "    def save_clusters(self):\n",
    "        filename = self.fname.replace(\".txt\", \"_clusters.json\")\n",
    "        return save_json(self.clusters, CLUSTER_DIR, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a02576",
   "metadata": {},
   "source": [
    "### ---> PROCESSING PIPELINE - steps 1 to 4 for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### MAIN PROCESSING PIPELINE\n",
    "def process_file(fname):\n",
    "    \"\"\"Process a single file through the complete pipeline.\"\"\"\n",
    "    print(f\"\\n\\n=============== PROCESSING FILE: {fname} ===============\")\n",
    "\n",
    "    # Step 1: Read file\n",
    "    with open(os.path.join(FOLDER_PATH, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Step 2: Create chunks\n",
    "    chunks, metadata = chunk_text(text, fname, inspect=True)        # Split text into chunks\n",
    "    print(f\"\\n=== Created {len(chunks)} chunks\")\n",
    "\n",
    "    # Step 3: Store in vector database\n",
    "    vector_manager = VectorStoreManager()    # Initialize vector store\n",
    "    vector_manager.add_chunks(chunks, metadata)                     # Add chunks with embeddings\n",
    "    print(f\"\\n=== Stored {len(chunks)} chunks in Chroma DB\")\n",
    "    \n",
    "    # Step 4: Mark as processed\n",
    "    with open(LOG_PATH, \"a\") as log:                                # Log processed file\n",
    "        log.write(fname + \"\\n\")\n",
    "\n",
    "    # Step 5: Cluster chunks\n",
    "    print(f\"\\n=== CLUSTERING CHUNKS FROM {fname}\")\n",
    "    chunks, embeddings, metadatas = vector_manager.get_chunks_with_embeddings(fname) # Retrieve stored data\n",
    "\n",
    "    clusterizer = ChunkClusterizer(chunks, embeddings, metadatas, fname)             # Initialize clustering\n",
    "    clusterizer.cluster_chunks()                                            # Perform clustering\n",
    "    clusterizer.save_clusters()                                             # Save clusters\n",
    "\n",
    "\n",
    "def process_all_new():\n",
    "    \"\"\"Process all new files in the input directory.\"\"\"\n",
    "    new_files = get_new_files()                                     # Get list of unprocessed files\n",
    "    \n",
    "    if not new_files:                                               # Exit early if no new files\n",
    "        print(\"\\n=== No new files to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Found {len(new_files)} new files: {new_files}\")\n",
    "    \n",
    "    for fname in new_files:                                         # Process each new file\n",
    "        process_file(fname)\n",
    "    \n",
    "    print(f\"\\n=== Completed processing {len(new_files)} files\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FOR LATER: \"MAIN GUARD\" OR \"ENTRY POINT PATTERN\" \n",
    "# EXECUTION \n",
    "#if __name__ == \"__main__\":\n",
    "#    process_all_new()                                               # Run the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0211580",
   "metadata": {},
   "source": [
    "#### Option 1: Process all new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508ab95",
   "metadata": {},
   "source": [
    "#### Option 2: Process 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99430c",
   "metadata": {},
   "source": [
    "## III. Continue pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216f8cf",
   "metadata": {},
   "source": [
    "### 5. Read notes clustered by HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09051397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HDBSCAN clusters file from cluster_dir\n",
    "def read_note_clusters(fname):\n",
    "    \"\"\"Read HDBSCAN clusters file from cluster_dir\"\"\"\n",
    "    with open(os.path.join(CLUSTER_DIR, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        cluster_data = f.read()\n",
    "    return cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d816696",
   "metadata": {},
   "source": [
    "### 6. Define system prompts and initiate LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c1bc8",
   "metadata": {},
   "source": [
    "#### 6.1. Prompt: recluster and generate cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02965bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_RECLUSTER = \"\"\"\n",
    "You are a smart assistant helping organize fragmented personal notes.\n",
    "\n",
    "The notes were previously split into chunks and clustered semantically. Each original note has its own clusters, saved as JSON files inside: ./data/cluster_dir/. Your task is to refine those clusters and give them a name.\n",
    "\n",
    "1. REORGANIZE CLUSTERS PER FILE:\n",
    "- You may SPLIT or MERGE clusters WITHIN EACH FILE to better reflect topic boundaries.\n",
    "- Keep a balance when creating the new clusters. The topic should NOT be too granular (e.g., splitting KMeans into 3 separate clusters), but also not too broad (e.g., grouping all machine learning together).\n",
    "- The goal is to create meaningful, reusable sub-notes by topic. Each cluster should contain all content from the original note that belongs to that topic.\n",
    "- For example, all KMeans content from a note should be together, not scattered across separate clusters.\n",
    "- Imagine someone searching \"KMeans\": everything relevant from the note should be in a single rewritten cluster.\n",
    "\n",
    "2. CLUSTER REORGANIZATION OUTPUT:\n",
    "2.1. REASONING:\n",
    "- provide a short reasoning section explaining what you split, merged, or reassigned and why. Format it as normal text.\n",
    "2.2. \n",
    "- Then output A CLEAR JSON OBJECT, with label exactly like this:\n",
    "\n",
    "NOTE_RECLUSTERED:\n",
    "{ ... }\n",
    "\n",
    "2.3.\n",
    "- Then output a SECOND JSON OBJECT, with label exactly like this:\n",
    "\n",
    "CLUSTER_NAMES:\n",
    "{ ... }\n",
    "\n",
    "\n",
    "- NOTE_RECLUSTERED contains the new clustering structure, following the same format as the input cluster JSON.\n",
    "\n",
    "- CLUSTER_NAMES is a dictionary mapping new cluster IDs created during reclustering (e.g., \"cluster_0\") to topic names.\n",
    "\n",
    "3. FORMATTING RULES (STRICT):\n",
    "- Keep the reasoning at the top.\n",
    "- After reasoning, print CLUSTER_NAMES: followed by the RAW JSON object (NO code blocks, NO quotes, NO markdown).\n",
    "- Then print NOTE_RECLUSTERED: followed by the RAW JSON (same rules).\n",
    "- DO NOT use triple backticks, ```json, or any Markdown formatting. Just the plain, raw JSON object after the tag.\n",
    "- Make sure CLUSTER_NAMES and NOTE_RECLUSTERED appear exactly like shown. They are anchors for parsing.\n",
    "- CLUSTER_NAMES topic names must be FOLDER-SAFE (use underscores, no spaces or special chars).\n",
    "DO NOT use placeholder text like '...' inside JSON.\n",
    "DO NOT include comments like `// ...` inside JSON.\n",
    "Make sure all JSON is valid and can be parsed with json.loads().\n",
    "- Example:\n",
    "\n",
    "CLUSTER_NAMES:\n",
    "{ \"cluster_0\": \"unsupervised_learning\", ... }\n",
    "\n",
    "NOTE_RECLUSTERED:\n",
    "{ \"cluster_0\": [ ... ], ... }\n",
    "\n",
    "You will receive a single note cluster JSON. Analyze and return the result using the format above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73edd53b",
   "metadata": {},
   "source": [
    "#### 6.2. Prompt: rewrite subnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SYSTEM_PROMPT_REWRITE = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b287f",
   "metadata": {},
   "source": [
    "#### 6.3. Initiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ccfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Ollama\n",
    "def query_ollama(model: str, prompt: str) -> str:\n",
    "    \"\"\"Initiate Ollama\"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=prompt)\n",
    "    if stderr:\n",
    "        print(\"OLLAMA STDERR:\", stderr)\n",
    "    return stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6ce16",
   "metadata": {},
   "source": [
    "### 7. Run LLM: recluster + cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Read note clusters\n",
    "note_3_clusters = read_note_clusters(\"note_3_dt_r_ul_clusters.json\")\n",
    "\n",
    "# Step 6: Run query\n",
    "prompt = SYSTEM_PROMPT_RECLUSTER + \"\\n\\nHERE IS THE CLUSTER DATA:\\n\" + note_3_clusters + \"\\n\\nPLEASE PERFORM THE RECLUSTERING AND CLUSTER NAMING TASKS.\"\n",
    "response = query_ollama(\"mistral\", prompt)\n",
    "\n",
    "# Step 6.1: Print response for inspect\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c614c",
   "metadata": {},
   "source": [
    "### 8. Extract response and save CLUSTER_NAMES and NOTE_x_RECLUSTERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc505d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response_text):\n",
    "    \"\"\"Extract JSON objects from LLM response and save into JSON files \"\"\"\n",
    "    \n",
    "    # Extract CLUSTER_NAMES section\n",
    "    cluster_names_match = re.search(r'CLUSTER_NAMES:\\s*(\\{[^}]*\\})', response_text, re.DOTALL)\n",
    "    if cluster_names_match:\n",
    "        cluster_names_str = cluster_names_match.group(1)\n",
    "        try:\n",
    "            cluster_names = json.loads(cluster_names_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle potential formatting issues\n",
    "            cluster_names = eval(cluster_names_str)\n",
    "    else:\n",
    "        cluster_names = {}\n",
    "    \n",
    "    # Extract NOTE_RECLUSTERED section\n",
    "    note_reclustered_match = re.search(r'NOTE_RECLUSTERED:\\s*(\\{.*\\})', response_text, re.DOTALL)\n",
    "    if note_reclustered_match:\n",
    "        note_reclustered_str = note_reclustered_match.group(1)\n",
    "        try:\n",
    "            note_reclustered = json.loads(note_reclustered_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle potential formatting issues - more complex parsing needed\n",
    "            note_reclustered = parse_complex_json(note_reclustered_str)\n",
    "    else:\n",
    "        note_reclustered = {}\n",
    "    \n",
    "    return cluster_names, note_reclustered\n",
    "\n",
    "\n",
    "\n",
    "def save_to_files(cluster_names, note_reclustered, output_dir=LLM_RECLUSTER):\n",
    "    \"\"\"Save CLUSTER_NAMES and NOTE_RECLUSTERED to separate JSON files.\"\"\"\n",
    "       \n",
    "    # Save CLUSTER_NAMES\n",
    "    cluster_names_file = os.path.join(output_dir, \"cluster_names.json\")\n",
    "    with open(cluster_names_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cluster_names, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save NOTE_RECLUSTERED\n",
    "    note_reclustered_file = os.path.join(output_dir, \"note_reclustered.json\")\n",
    "    with open(note_reclustered_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(note_reclustered, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Files saved:\")\n",
    "    print(f\"  - {cluster_names_file}\")\n",
    "    print(f\"  - {note_reclustered_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def parse_complex_json(json_str):\n",
    "    \"\"\"Helper function to parse complex JSON with potential formatting issues.\"\"\"\n",
    "    try:\n",
    "        # Try direct JSON parsing first\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # If that fails, try eval (be careful with this in production)\n",
    "        try:\n",
    "            return eval(json_str)\n",
    "        except:\n",
    "            # If all else fails, return empty dict\n",
    "            return {}\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Parse the response\n",
    "    cluster_names, note_reclustered = parse_response(response)\n",
    "    \n",
    "    # Save to separate files\n",
    "    save_to_files(cluster_names, note_reclustered)\n",
    "    \n",
    "    print(\"CLUSTER_NAMES:\")\n",
    "    print(json.dumps(cluster_names, indent=2))\n",
    "    print(\"\\nNOTE_RECLUSTERED:\")\n",
    "    print(json.dumps(note_reclustered, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7b833",
   "metadata": {},
   "source": [
    "### 9. Generate sub-notes from each cluster - raw notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clusters_to_files(json_file_path, output_folder=SUBNOTES_RAW):\n",
    "    \"\"\" Split JSON file by cluster and save into new files \"\"\"\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster_name, chunks in data.items():\n",
    "        # Create the output file path\n",
    "        output_file = os.path.join(output_folder, f\"{cluster_name}.txt\")\n",
    "        \n",
    "        # Format the content with simple bullet points\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Topic: {cluster_name}\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            # Write each chunk as a simple bullet point\n",
    "            for chunk_data in chunks:\n",
    "                chunk_text = chunk_data.get('chunk', '').strip()\n",
    "                if chunk_text:  # Only write non-empty chunks\n",
    "                    f.write(f\"• {chunk_text}\\n\")\n",
    "            \n",
    "            f.write(f\"\\n[Total: {len(chunks)} notes]\\n\")\n",
    "        \n",
    "        print(f\"✓ Created {output_file} with {len(chunks)} chunks\")\n",
    "    \n",
    "    print(f\"\\nAll cluster files saved to '{output_folder}' folder\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your JSON file path\n",
    "    json_file_path = \"./data/outputs/LLM_recluster/note_reclustered.json\"\n",
    "    \n",
    "    split_clusters_to_files(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cdef4",
   "metadata": {},
   "source": [
    "### 10. Rewrite subnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prompt to rewrite subnotes \n",
    "\n",
    "# Save the subnotes in FINAL_NOTES directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
